{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c7b589a3-8115-4880-ab8b-1697ec4f5da7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-23 22:52:27.253231: I metal_plugin/src/device/metal_device.cc:1154] Metal device set to: Apple M2\n",
      "2025-09-23 22:52:27.253365: I metal_plugin/src/device/metal_device.cc:296] systemMemory: 16.00 GB\n",
      "2025-09-23 22:52:27.253372: I metal_plugin/src/device/metal_device.cc:313] maxCacheSize: 5.33 GB\n",
      "2025-09-23 22:52:27.253403: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:305] Could not identify NUMA node of platform GPU ID 0, defaulting to 0. Your kernel may not have been built with NUMA support.\n",
      "2025-09-23 22:52:27.253416: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:271] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 0 MB memory) -> physical PluggableDevice (device: 0, name: METAL, pci bus id: <undefined>)\n",
      "2025-09-23 22:52:28.144243: W tensorflow/core/framework/local_rendezvous.cc:404] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n",
      "2025-09-23 22:52:28.319216: W tensorflow/core/framework/local_rendezvous.cc:404] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resultados individuales:\n",
      "      GRU -> log_cosh=1.291796 | AUTC=0.064105\n",
      "      CNN -> log_cosh=1.391858 | AUTC=0.041784\n",
      "    TRANS -> log_cosh=1.587197 | AUTC=0.016853\n",
      "\n",
      "Resultados ensembles:\n",
      "              simple -> log_cosh=1.205373 | AUTC=0.065723\n",
      "       ponderado_log -> log_cosh=1.200035 | AUTC=0.062822\n",
      "      ponderado_autc -> log_cosh=1.192552 | AUTC=0.075731\n",
      "    mix_25log_75autc -> log_cosh=1.192238 | AUTC=0.080158\n",
      "    mix_50log_50autc -> log_cosh=1.193385 | AUTC=0.082240\n",
      "    mix_75log_25autc -> log_cosh=1.195987 | AUTC=0.073073\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-23 22:52:28.581177: W tensorflow/core/framework/local_rendezvous.cc:404] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# 1) Carga de datos, escalado y construcción de test_dss (20→1)\n",
    "# ============================================================\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from joblib import load, dump\n",
    "\n",
    "# --- Carga CSV ---\n",
    "df = pd.read_csv('./data/cierres_diarios_2005_2025n.csv',\n",
    "                 parse_dates=['Date'], index_col='Date')\n",
    "\n",
    "# --- Limpieza nulos ---\n",
    "df.ffill(inplace=True)\n",
    "df.bfill(inplace=True)\n",
    "\n",
    "# --- Split en crudo (antes de escalar) ---\n",
    "n = len(df)\n",
    "train_raw = df.iloc[:int(n*0.7)]\n",
    "val_raw   = df.iloc[int(n*0.7):int(n*0.9)]\n",
    "test_raw  = df.iloc[int(n*0.9):]\n",
    "\n",
    "# --- Reusar scaler si existe; si no, ajustarlo SOLO con train ---\n",
    "scaler_path = \"scaler_modelos.joblib\"\n",
    "if os.path.exists(scaler_path):\n",
    "    scaler = load(scaler_path)\n",
    "else:\n",
    "    scaler = MinMaxScaler()\n",
    "    scaler.fit(train_raw)\n",
    "    dump(scaler, scaler_path)\n",
    "\n",
    "# --- Transformar splits con el MISMO scaler ---\n",
    "train_df = pd.DataFrame(scaler.transform(train_raw), index=train_raw.index, columns=train_raw.columns).astype(\"float32\")\n",
    "val_df   = pd.DataFrame(scaler.transform(val_raw),   index=val_raw.index,   columns=val_raw.columns).astype(\"float32\")\n",
    "test_df  = pd.DataFrame(scaler.transform(test_raw),  index=test_raw.index,  columns=test_raw.columns).astype(\"float32\")\n",
    "\n",
    "# --- Helper para datasets (ventana->horizonte) ---\n",
    "def make_dataset(data, window_size, horizon, batch_size=32, shuffle=True):\n",
    "    ds = tf.keras.preprocessing.timeseries_dataset_from_array(\n",
    "        data=data.values,\n",
    "        targets=None,\n",
    "        sequence_length=window_size + horizon,\n",
    "        sequence_stride=1,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=shuffle\n",
    "    )\n",
    "    return ds.map(\n",
    "        lambda seq: (\n",
    "            tf.cast(seq[:, :window_size, :], tf.float32),\n",
    "            tf.cast(seq[:, window_size:, :], tf.float32)\n",
    "        )\n",
    "    )\n",
    "\n",
    "# --- Construcción S (20→1) ---\n",
    "train_dss = make_dataset(train_df, window_size=20, horizon=1, batch_size=32, shuffle=True)\n",
    "val_dss   = make_dataset(val_df,   window_size=20, horizon=1, batch_size=32, shuffle=False)\n",
    "test_dss  = make_dataset(test_df,  window_size=20, horizon=1, batch_size=32, shuffle=False)  # <- el que vamos a usar\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# 2) Utilidades métricas (log_cosh y AUTC)\n",
    "# ============================================================\n",
    "eps_list = [0.005, 0.01, 0.02, 0.05, 0.1]\n",
    "\n",
    "def _eps_tag(e: float) -> str:\n",
    "    s = f\"{e:.6f}\".rstrip(\"0\").rstrip(\".\")\n",
    "    return s.replace(\".\", \"_\")\n",
    "\n",
    "def _get_metric_value_relaxed(res: dict, base: str) -> float:\n",
    "    if base in res:\n",
    "        return res[base]\n",
    "    for k in res.keys():\n",
    "        if k.startswith(base):\n",
    "            return res[k]\n",
    "    return float(\"nan\")\n",
    "\n",
    "def compute_autc_from_results(res: dict, eps_list) -> float:\n",
    "    eps = np.array(sorted(eps_list), dtype=np.float64)\n",
    "    acc, missing = [], []\n",
    "    for e in eps:\n",
    "        base = f\"within_eps_{_eps_tag(e)}\"\n",
    "        val = _get_metric_value_relaxed(res, base)\n",
    "        acc.append(val)\n",
    "        if not np.isfinite(val):\n",
    "            missing.append(base)\n",
    "    acc = np.array(acc, dtype=np.float64)\n",
    "    mask = np.isfinite(acc)\n",
    "    if mask.sum() < 2:\n",
    "        return float(\"nan\")\n",
    "    return float(np.trapz(acc[mask], eps[mask]) / (eps[mask][-1] - eps[mask][0]))\n",
    "\n",
    "def log_cosh_np(y_true: np.ndarray, y_pred: np.ndarray) -> float:\n",
    "    return float(np.mean(np.log(np.cosh(y_pred - y_true))))\n",
    "\n",
    "def within_eps_dict(y_true: np.ndarray, y_pred: np.ndarray, eps_list) -> dict:\n",
    "    res = {}\n",
    "    abs_err = np.abs(y_pred - y_true)  # soporta (N,H,F)\n",
    "    for e in eps_list:\n",
    "        res[f\"within_eps_{_eps_tag(e)}\"] = float(np.mean(abs_err <= e))\n",
    "    return res\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# 3) Evaluación de modelos y ensembles\n",
    "# ============================================================\n",
    "def evaluate_model(model: tf.keras.Model, test_ds: tf.data.Dataset, eps_list):\n",
    "    y_true_list, y_pred_list = [], []\n",
    "    for xb, yb in test_ds:\n",
    "        y_true_list.append(yb.numpy())\n",
    "        y_pred_list.append(model(xb, training=False).numpy())\n",
    "    y_true = np.concatenate(y_true_list, axis=0)\n",
    "    y_pred = np.concatenate(y_pred_list, axis=0)\n",
    "\n",
    "    res = within_eps_dict(y_true, y_pred, eps_list)\n",
    "    return {\n",
    "        \"log_cosh\": log_cosh_np(y_true, y_pred),\n",
    "        \"AUTC\": compute_autc_from_results(res, eps_list),\n",
    "        \"within\": res,\n",
    "        \"y_true\": y_true,\n",
    "        \"y_pred\": y_pred,\n",
    "    }\n",
    "\n",
    "def weighted_average(preds_list, weights):\n",
    "    w = np.array(weights, dtype=np.float64)\n",
    "    w = w / w.sum()\n",
    "    stacked = np.stack(preds_list, axis=0)  # (3, N, H, F)\n",
    "    return np.tensordot(w, stacked, axes=(0, 0))  # -> (N, H, F)\n",
    "\n",
    "def mix_weights(res_list, alpha_log: float):\n",
    "    # log_cosh: menor es mejor => usar inverso\n",
    "    log_vec  = np.array([1/r[\"log_cosh\"] for r in res_list], dtype=np.float64)\n",
    "    autc_vec = np.array([r[\"AUTC\"]       for r in res_list], dtype=np.float64)\n",
    "    log_w  = log_vec  / log_vec.sum()\n",
    "    autc_w = autc_vec / autc_vec.sum()\n",
    "    return alpha_log * log_w + (1.0 - alpha_log) * autc_w\n",
    "\n",
    "def run_all_ensembles(model_gru, model_cnn, model_trans, test_dss, eps_list):\n",
    "    # 1) individuales\n",
    "    r_gru  = evaluate_model(model_gru,  test_dss, eps_list)\n",
    "    r_cnn  = evaluate_model(model_cnn,  test_dss, eps_list)\n",
    "    r_tran = evaluate_model(model_trans, test_dss, eps_list)\n",
    "\n",
    "    y_true  = r_gru[\"y_true\"]\n",
    "    preds   = [r_gru[\"y_pred\"], r_cnn[\"y_pred\"], r_tran[\"y_pred\"]]\n",
    "    reslist = [r_gru, r_cnn, r_tran]\n",
    "\n",
    "    # 2) esquemas de pesos\n",
    "    schemes = {\n",
    "        \"simple\":               [1.0, 1.0, 1.0],\n",
    "        \"ponderado_log\":        [1/r_gru[\"log_cosh\"], 1/r_cnn[\"log_cosh\"], 1/r_tran[\"log_cosh\"]],\n",
    "        \"ponderado_autc\":       [r_gru[\"AUTC\"], r_cnn[\"AUTC\"], r_tran[\"AUTC\"]],\n",
    "        \"mix_25log_75autc\":     mix_weights(reslist, alpha_log=0.25),\n",
    "        \"mix_50log_50autc\":     mix_weights(reslist, alpha_log=0.50),\n",
    "        \"mix_75log_25autc\":     mix_weights(reslist, alpha_log=0.75),\n",
    "    }\n",
    "\n",
    "    def eval_from_pred(y_true, y_pred):\n",
    "        res = within_eps_dict(y_true, y_pred, eps_list)\n",
    "        return {\"log_cosh\": log_cosh_np(y_true, y_pred),\n",
    "                \"AUTC\": compute_autc_from_results(res, eps_list)}\n",
    "\n",
    "    results = {\n",
    "        \"individual\": {\n",
    "            \"GRU\":  {\"log_cosh\": r_gru[\"log_cosh\"],  \"AUTC\": r_gru[\"AUTC\"]},\n",
    "            \"CNN\":  {\"log_cosh\": r_cnn[\"log_cosh\"],  \"AUTC\": r_cnn[\"AUTC\"]},\n",
    "            \"TRANS\":{\"log_cosh\": r_tran[\"log_cosh\"], \"AUTC\": r_tran[\"AUTC\"]},\n",
    "        },\n",
    "        \"ensembles\": {}\n",
    "    }\n",
    "\n",
    "    for name, w in schemes.items():\n",
    "        y_pred_ens = weighted_average(preds, w)\n",
    "        results[\"ensembles\"][name] = eval_from_pred(y_true, y_pred_ens)\n",
    "\n",
    "    return results\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# EJEMPLO DE USO\n",
    "# (Asegúrate de tener ya construidos test_dss = make_dataset(..., window_size=20, horizon=1, shuffle=False))\n",
    "# Y de que los datos estén escalados con EL MISMO scaler que usaste al entrenar los tres modelos.\n",
    "# ============================================================\n",
    "from tensorflow.keras.models import load_model \n",
    "path_gru = \"./models_gru_huber_sweep/gru_huber_w20_h1_delta0_01735741273_S.keras\" \n",
    "model_gru = load_model(path_gru, compile=False)\n",
    "path_cnn = \"./models_cnn_huber_sweep/cnn_huber_w20_h1_delta0_04116208553_S.keras\" \n",
    "model_cnn = load_model(path_cnn, compile=False)\n",
    "path_trans = \"./models_transformer_huber_sweep/transformer_huber_w20_h1_delta0_04116208553_S.keras\" \n",
    "model_trans = load_model(path_trans, compile=False)\n",
    "\n",
    "# Ejecutar:\n",
    "results = run_all_ensembles(model_gru, model_cnn, model_trans, test_dss, eps_list)\n",
    "\n",
    "# Mostrar resultados de forma legible:\n",
    "print(\"Resultados individuales:\")\n",
    "for k, v in results[\"individual\"].items():\n",
    "    print(f\"{k:>9} -> log_cosh={v['log_cosh']:.6f} | AUTC={v['AUTC']:.6f}\")\n",
    "\n",
    "print(\"\\nResultados ensembles:\")\n",
    "for k, v in results[\"ensembles\"].items():\n",
    "    print(f\"{k:>20} -> log_cosh={v['log_cosh']:.6f} | AUTC={v['AUTC']:.6f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dd870cc-432b-4df9-82ab-115099da7f0f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
