{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7773d2a8",
   "metadata": {},
   "source": [
    "# Escenario M (w=60 → h=5)\n",
    "\n",
    "# Evaluación de Ensembles (GRU, CNN, Transformer)\n",
    "\n",
    "Este cuaderno:\n",
    "- Carga datos y reutiliza el mismo **MinMaxScaler** ajustado con *train*.\n",
    "- Construye el dataset de **test** para el escenario especificado.\n",
    "- Calcula métricas **log_cosh** y **AUTC** para cada modelo individual.\n",
    "- Evalúa 6 esquemas de **promediado** (simple, ponderado por `log_cosh`, por `AUTC`, y mezclas 25/75, 50/50, 75/25).\n",
    "- Imprime una tabla con resultados.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "704b8c05",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, glob\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from joblib import load, dump\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1c24d95f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Parámetros ---\n",
    "CSV_PATH = './data/cierres_diarios_2005_2025n.csv'\n",
    "SCALER_PATH = 'scaler_modelos.joblib'\n",
    "\n",
    "# --- Carga CSV ---\n",
    "df = pd.read_csv(CSV_PATH, parse_dates=['Date'], index_col='Date')\n",
    "\n",
    "# --- Limpieza nulos ---\n",
    "df.ffill(inplace=True)\n",
    "df.bfill(inplace=True)\n",
    "\n",
    "# --- Split en crudo (antes de escalar) ---\n",
    "n = len(df)\n",
    "train_raw = df.iloc[:int(n*0.7)]\n",
    "val_raw   = df.iloc[int(n*0.7):int(n*0.9)]\n",
    "test_raw  = df.iloc[int(n*0.9):]\n",
    "\n",
    "# --- Reusar scaler si existe; si no, ajustarlo SOLO con train ---\n",
    "if os.path.exists(SCALER_PATH):\n",
    "    scaler = load(SCALER_PATH)\n",
    "else:\n",
    "    scaler = MinMaxScaler()\n",
    "    scaler.fit(train_raw)\n",
    "    dump(scaler, SCALER_PATH)\n",
    "\n",
    "# --- Transformar splits con el MISMO scaler ---\n",
    "train_df = pd.DataFrame(scaler.transform(train_raw), index=train_raw.index, columns=train_raw.columns).astype(\"float32\")\n",
    "val_df   = pd.DataFrame(scaler.transform(val_raw),   index=val_raw.index,   columns=val_raw.columns).astype(\"float32\")\n",
    "test_df  = pd.DataFrame(scaler.transform(test_raw),  index=test_raw.index,  columns=test_raw.columns).astype(\"float32\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a231a00b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_dataset(data, window_size, horizon, batch_size=32, shuffle=True):\n",
    "    ds = tf.keras.preprocessing.timeseries_dataset_from_array(\n",
    "        data=data.values,\n",
    "        targets=None,\n",
    "        sequence_length=window_size + horizon,\n",
    "        sequence_stride=1,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=shuffle\n",
    "    )\n",
    "    return ds.map(\n",
    "        lambda seq: (\n",
    "            tf.cast(seq[:, :window_size, :], tf.float32),\n",
    "            tf.cast(seq[:, window_size:, :], tf.float32)\n",
    "        )\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cb4a19b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Métricas (log_cosh + AUTC) ---\n",
    "eps_list = [0.005, 0.01, 0.02, 0.05, 0.1]\n",
    "\n",
    "def _eps_tag(e: float) -> str:\n",
    "    s = f\"{e:.6f}\".rstrip(\"0\").rstrip(\".\")\n",
    "    return s.replace(\".\", \"_\")\n",
    "\n",
    "def _get_metric_value_relaxed(res: dict, base: str) -> float:\n",
    "    if base in res:\n",
    "        return res[base]\n",
    "    for k in res.keys():\n",
    "        if k.startswith(base):\n",
    "            return res[k]\n",
    "    return float(\"nan\")\n",
    "\n",
    "def compute_autc_from_results(res: dict, eps_list) -> float:\n",
    "    eps = np.array(sorted(eps_list), dtype=np.float64)\n",
    "    acc, missing = [], []\n",
    "    for e in eps:\n",
    "        base = f\"within_eps_{_eps_tag(e)}\"\n",
    "        val = _get_metric_value_relaxed(res, base)\n",
    "        acc.append(val)\n",
    "        if not np.isfinite(val):\n",
    "            missing.append(base)\n",
    "    acc = np.array(acc, dtype=np.float64)\n",
    "    mask = np.isfinite(acc)\n",
    "    if mask.sum() < 2:\n",
    "        return float(\"nan\")\n",
    "    return float(np.trapz(acc[mask], eps[mask]) / (eps[mask][-1] - eps[mask][0]))\n",
    "\n",
    "def log_cosh_np(y_true: np.ndarray, y_pred: np.ndarray) -> float:\n",
    "    return float(np.mean(np.log(np.cosh(y_pred - y_true))))\n",
    "\n",
    "def within_eps_dict(y_true: np.ndarray, y_pred: np.ndarray, eps_list) -> dict:\n",
    "    res = {}\n",
    "    abs_err = np.abs(y_pred - y_true)  # soporta (N,H,F)\n",
    "    for e in eps_list:\n",
    "        res[f\"within_eps_{_eps_tag(e)}\"] = float(np.mean(abs_err <= e))\n",
    "    return res\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e74c2209",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model: tf.keras.Model, test_ds: tf.data.Dataset, eps_list):\n",
    "    y_true_list, y_pred_list = [], []\n",
    "    for xb, yb in test_ds:\n",
    "        y_true_list.append(yb.numpy())\n",
    "        y_pred_list.append(model(xb, training=False).numpy())\n",
    "    y_true = np.concatenate(y_true_list, axis=0)\n",
    "    y_pred = np.concatenate(y_pred_list, axis=0)\n",
    "\n",
    "    res = within_eps_dict(y_true, y_pred, eps_list)\n",
    "    return {\n",
    "        \"log_cosh\": log_cosh_np(y_true, y_pred),\n",
    "        \"AUTC\": compute_autc_from_results(res, eps_list),\n",
    "        \"within\": res,\n",
    "        \"y_true\": y_true,\n",
    "        \"y_pred\": y_pred,\n",
    "    }\n",
    "\n",
    "def weighted_average(preds_list, weights):\n",
    "    w = np.array(weights, dtype=np.float64)\n",
    "    w = w / w.sum()\n",
    "    stacked = np.stack(preds_list, axis=0)  # (3, N, H, F)\n",
    "    return np.tensordot(w, stacked, axes=(0, 0))  # -> (N, H, F)\n",
    "\n",
    "def mix_weights(res_list, alpha_log: float):\n",
    "    # log_cosh: menor es mejor => usar inverso\n",
    "    log_vec  = np.array([1/r[\"log_cosh\"] for r in res_list], dtype=np.float64)\n",
    "    autc_vec = np.array([r[\"AUTC\"]       for r in res_list], dtype=np.float64)\n",
    "    log_w  = log_vec  / log_vec.sum()\n",
    "    autc_w = autc_vec / autc_vec.sum()\n",
    "    return alpha_log * log_w + (1.0 - alpha_log) * autc_w\n",
    "\n",
    "def run_all_ensembles(model_gru, model_cnn, model_trans, test_ds, eps_list):\n",
    "    r_gru  = evaluate_model(model_gru,  test_ds, eps_list)\n",
    "    r_cnn  = evaluate_model(model_cnn,  test_ds, eps_list)\n",
    "    r_tran = evaluate_model(model_trans, test_ds, eps_list)\n",
    "\n",
    "    y_true  = r_gru[\"y_true\"]\n",
    "    preds   = [r_gru[\"y_pred\"], r_cnn[\"y_pred\"], r_tran[\"y_pred\"]]\n",
    "    reslist = [r_gru, r_cnn, r_tran]\n",
    "\n",
    "    schemes = {\n",
    "        \"simple\":               [1.0, 1.0, 1.0],\n",
    "        \"ponderado_log\":        [1/r_gru[\"log_cosh\"], 1/r_cnn[\"log_cosh\"], 1/r_tran[\"log_cosh\"]],\n",
    "        \"ponderado_autc\":       [r_gru[\"AUTC\"], r_cnn[\"AUTC\"], r_tran[\"AUTC\"]],\n",
    "        \"mix_25log_75autc\":     mix_weights(reslist, alpha_log=0.25),\n",
    "        \"mix_50log_50autc\":     mix_weights(reslist, alpha_log=0.50),\n",
    "        \"mix_75log_25autc\":     mix_weights(reslist, alpha_log=0.75),\n",
    "    }\n",
    "\n",
    "    def eval_from_pred(y_true, y_pred):\n",
    "        res = within_eps_dict(y_true, y_pred, eps_list)\n",
    "        return {\"log_cosh\": log_cosh_np(y_true, y_pred),\n",
    "                \"AUTC\": compute_autc_from_results(res, eps_list)}\n",
    "\n",
    "    results = {\n",
    "        \"individual\": {\n",
    "            \"GRU\":  {\"log_cosh\": r_gru[\"log_cosh\"],  \"AUTC\": r_gru[\"AUTC\"]},\n",
    "            \"CNN\":  {\"log_cosh\": r_cnn[\"log_cosh\"],  \"AUTC\": r_cnn[\"AUTC\"]},\n",
    "            \"TRANS\":{\"log_cosh\": r_tran[\"log_cosh\"], \"AUTC\": r_tran[\"AUTC\"]},\n",
    "        },\n",
    "        \"ensembles\": {}\n",
    "    }\n",
    "\n",
    "    for name, w in schemes.items():\n",
    "        y_pred_ens = weighted_average(preds, w)\n",
    "        results[\"ensembles\"][name] = eval_from_pred(y_true, y_pred_ens)\n",
    "\n",
    "    return results\n",
    "\n",
    "def pick_model(pattern: str) -> str:\n",
    "    matches = glob.glob(pattern)\n",
    "    if not matches:\n",
    "        raise FileNotFoundError(f\"No se encontraron modelos con patrón: {pattern}\")\n",
    "    matches.sort(key=lambda p: os.path.getmtime(p), reverse=True)\n",
    "    return matches[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "488e82d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-23 23:15:34.724814: I metal_plugin/src/device/metal_device.cc:1154] Metal device set to: Apple M2\n",
      "2025-09-23 23:15:34.724874: I metal_plugin/src/device/metal_device.cc:296] systemMemory: 16.00 GB\n",
      "2025-09-23 23:15:34.724892: I metal_plugin/src/device/metal_device.cc:313] maxCacheSize: 5.33 GB\n",
      "2025-09-23 23:15:34.724935: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:305] Could not identify NUMA node of platform GPU ID 0, defaulting to 0. Your kernel may not have been built with NUMA support.\n",
      "2025-09-23 23:15:34.724965: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:271] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 0 MB memory) -> physical PluggableDevice (device: 0, name: METAL, pci bus id: <undefined>)\n",
      "2025-09-23 23:15:35.635152: W tensorflow/core/framework/local_rendezvous.cc:404] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n",
      "2025-09-23 23:15:35.970057: W tensorflow/core/framework/local_rendezvous.cc:404] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resultados individuales (M - w60 h5):\n",
      "      GRU -> log_cosh=1.347702 | AUTC=0.047233\n",
      "      CNN -> log_cosh=1.466199 | AUTC=0.042731\n",
      "    TRANS -> log_cosh=1.761740 | AUTC=0.013818\n",
      "\n",
      "Resultados ensembles (M):\n",
      "              simple -> log_cosh=1.286618 | AUTC=0.068660\n",
      "       ponderado_log -> log_cosh=1.274581 | AUTC=0.073878\n",
      "      ponderado_autc -> log_cosh=1.248080 | AUTC=0.077785\n",
      "    mix_25log_75autc -> log_cosh=1.251745 | AUTC=0.074718\n",
      "    mix_50log_50autc -> log_cosh=1.257388 | AUTC=0.072351\n",
      "    mix_75log_25autc -> log_cosh=1.265005 | AUTC=0.073756\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-23 23:15:36.227482: W tensorflow/core/framework/local_rendezvous.cc:404] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n"
     ]
    }
   ],
   "source": [
    "# === Escenario M: ventana=60, horizonte=5 ===\n",
    "train_dsm = make_dataset(train_df, window_size=60, horizon=5, batch_size=32, shuffle=True)\n",
    "val_dsm   = make_dataset(val_df,   window_size=60, horizon=5, batch_size=32, shuffle=False)\n",
    "test_dsm  = make_dataset(test_df,  window_size=60, horizon=5, batch_size=32, shuffle=False)  # <- usar éste\n",
    "\n",
    "gru_path  = pick_model(\"./models_gru_huber_sweep/gru_huber_w60_h5_delta0_1_M.keras\")\n",
    "cnn_path  = pick_model(\"./models_cnn_huber_sweep/cnn_huber_w60_h5_delta0_02_M.keras\")\n",
    "tran_path = pick_model(\"./models_transformer_huber_sweep/transformer_huber_w60_h5_delta0_1_M.keras\")\n",
    "\n",
    "model_gru   = tf.keras.models.load_model(gru_path,  compile=False)\n",
    "model_cnn   = tf.keras.models.load_model(cnn_path,  compile=False)\n",
    "model_trans = tf.keras.models.load_model(tran_path, compile=False)\n",
    "\n",
    "results_M = run_all_ensembles(model_gru, model_cnn, model_trans, test_dsm, eps_list)\n",
    "\n",
    "print(\"Resultados individuales (M - w60 h5):\")\n",
    "for k, v in results_M[\"individual\"].items():\n",
    "    print(f\"{k:>9} -> log_cosh={v['log_cosh']:.6f} | AUTC={v['AUTC']:.6f}\")\n",
    "\n",
    "print(\"\\nResultados ensembles (M):\")\n",
    "for k, v in results_M[\"ensembles\"].items():\n",
    "    print(f\"{k:>20} -> log_cosh={v['log_cosh']:.6f} | AUTC={v['AUTC']:.6f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8be35bc-9d1b-4397-b167-3f156edf0a7d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
