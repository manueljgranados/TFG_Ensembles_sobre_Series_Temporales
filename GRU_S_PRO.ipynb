{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "20a1a11b-4c15-4e70-9e9e-58b6de7075f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df1 = pd.read_csv('./data/cierres_diarios_S1.csv', parse_dates=['Date'], index_col='Date')\n",
    "df2 = pd.read_csv('./data/cierres_diarios_S2.csv', parse_dates=['Date'], index_col='Date')\n",
    "df3 = pd.read_csv('./data/cierres_diarios_S3.csv', parse_dates=['Date'], index_col='Date')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3f2b841c-68a6-46d1-a1e8-e9239ed0fdbd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-21 19:57:29.387933: I metal_plugin/src/device/metal_device.cc:1154] Metal device set to: Apple M2\n",
      "2025-09-21 19:57:29.387951: I metal_plugin/src/device/metal_device.cc:296] systemMemory: 16.00 GB\n",
      "2025-09-21 19:57:29.387956: I metal_plugin/src/device/metal_device.cc:313] maxCacheSize: 5.33 GB\n",
      "2025-09-21 19:57:29.387968: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:305] Could not identify NUMA node of platform GPU ID 0, defaulting to 0. Your kernel may not have been built with NUMA support.\n",
      "2025-09-21 19:57:29.387977: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:271] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 0 MB memory) -> physical PluggableDevice (device: 0, name: METAL, pci bus id: <undefined>)\n",
      "2025-09-21 19:57:29.782085: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:117] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Resultados S1 (20→1) ===\n",
      "  Huber (loss):              0.057957\n",
      "  log_cosh:                  2.930186\n",
      "  within_eps_0_005          : 0.040000\n",
      "  within_eps_0_01           : 0.040000\n",
      "  within_eps_0_02           : 0.040000\n",
      "  within_eps_0_05           : 0.040000\n",
      "  within_eps_0_1            : 0.040000\n",
      "  AUTC[0.005–0.100]:         0.040000\n",
      "\n",
      "=== Resultados S2 (20→1) ===\n",
      "  Huber (loss):              0.052172\n",
      "  log_cosh:                  2.599282\n",
      "  within_eps_0_005          : 0.000000\n",
      "  within_eps_0_01           : 0.000000\n",
      "  within_eps_0_02           : 0.040000\n",
      "  within_eps_0_05           : 0.120000\n",
      "  within_eps_0_1            : 0.120000\n",
      "  AUTC[0.005–0.100]:         0.090526\n",
      "\n",
      "=== Resultados S3 (20→1) ===\n",
      "  Huber (loss):              0.053469\n",
      "  log_cosh:                  2.663874\n",
      "  within_eps_0_005          : 0.000000\n",
      "  within_eps_0_01           : 0.000000\n",
      "  within_eps_0_02           : 0.000000\n",
      "  within_eps_0_05           : 0.040000\n",
      "  within_eps_0_1            : 0.120000\n",
      "  AUTC[0.005–0.100]:         0.048421\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\ndef predict_and_print_prices(tag, df_scaled, tickers_show=(\"^GSPC\",\"^IXIC\",\"^IBEX\",\"^VIX\")):\\n    X_win, y_true_next = single_window_arrays(df_scaled, w=window_size, h=horizon)\\n    y_pred_scaled = model_s.predict(X_win, verbose=0)                 # (1,1,F)\\n    # Desescalar\\n    y_pred = scaler.inverse_transform(y_pred_scaled.reshape(1, -1)).reshape(1,1,-1)\\n    y_real = scaler.inverse_transform(y_true_next.reshape(1, -1)).reshape(1,1,-1)\\n\\n    print(f\"\\nPredicción vs Real (precio de cierre) {tag}:\")\\n    for t in tickers_show:\\n        j = feature_order.index(t)\\n        print(f\"  {t:10s}  pred: {float(y_pred[0,0,j]):.4f}   real: {float(y_real[0,0,j]):.4f}\")\\n\\npredict_and_print_prices(\"S1\", df1_scaled)\\npredict_and_print_prices(\"S2\", df2_scaled)\\npredict_and_print_prices(\"S3\", df3_scaled)\\n'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Preparación de los datos\n",
    "\n",
    "# Eliminación de nulos\n",
    "df1.ffill(inplace=True) # rellena con el ultimo precio conocido\n",
    "df1.bfill(inplace=True) # por si hay alguna celda con NaN al principio\n",
    "df2.ffill(inplace=True) \n",
    "df2.bfill(inplace=True) \n",
    "df3.ffill(inplace=True) \n",
    "df3.bfill(inplace=True)\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from joblib import load\n",
    "from typing import List\n",
    "\n",
    "# Cargar scaler y columnas base (mismo orden que en entrenamiento)\n",
    "scaler = load(\"scaler_modelos.joblib\")\n",
    "\n",
    "\n",
    "# Como el scaler tiene 'feature_names_in_', usamos ese orden:\n",
    "feature_order = list(scaler.feature_names_in_)\n",
    "\n",
    "\n",
    "# Filtramos/ordenamos exactamente como en entrenamiento\n",
    "df1 = df1[feature_order].copy()\n",
    "df2 = df2[feature_order].copy()\n",
    "df3 = df3[feature_order].copy()\n",
    "\n",
    "\n",
    "# Escalar con el mismo scaler\n",
    "df1_scaled = pd.DataFrame(\n",
    "    scaler.transform(df1[feature_order]),\n",
    "    index=df1.index,\n",
    "    columns=feature_order\n",
    ").astype(\"float32\")\n",
    "\n",
    "df2_scaled = pd.DataFrame(\n",
    "    scaler.transform(df2[feature_order]),\n",
    "    index=df2.index,\n",
    "    columns=feature_order\n",
    ").astype(\"float32\")\n",
    "\n",
    "df3_scaled = pd.DataFrame(\n",
    "    scaler.transform(df3[feature_order]),\n",
    "    index=df3.index,\n",
    "    columns=feature_order\n",
    ").astype(\"float32\")\n",
    "\n",
    "# Carga del modelo \n",
    "from tensorflow.keras.models import load_model \n",
    "path_s = \"./models_gru_huber_sweep/gru_huber_w20_h1_delta0_01735741273_S.keras\" \n",
    "model_s = load_model(path_s, compile=False) \n",
    "delta_s = 0.017357412725687027\n",
    "\n",
    "\n",
    "# === Datasets coherentes con el entrenamiento: usar SIEMPRE datos ESCALADOS ===\n",
    "def make_dataset(data_scaled, window_size, horizon, batch_size=32, shuffle=False):\n",
    "    ds = tf.keras.preprocessing.timeseries_dataset_from_array(\n",
    "        data=data_scaled.values,\n",
    "        targets=None,\n",
    "        sequence_length=window_size + horizon,\n",
    "        sequence_stride=1,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=shuffle\n",
    "    )\n",
    "    return ds.map(\n",
    "        lambda seq: (\n",
    "            tf.cast(seq[:, :window_size, :], tf.float32),   # X\n",
    "            tf.cast(seq[:, window_size:, :], tf.float32)    # y\n",
    "        )\n",
    "    )\n",
    "\n",
    "window_size = 20\n",
    "horizon = 1\n",
    "\n",
    "# Evaluar SOLO el primer (20->1) del intervalo:\n",
    "ds1 = make_dataset(df1_scaled.iloc[:window_size+horizon], window_size, horizon, batch_size=1, shuffle=False)\n",
    "ds2 = make_dataset(df2_scaled.iloc[:window_size+horizon], window_size, horizon, batch_size=1, shuffle=False)\n",
    "ds3 = make_dataset(df3_scaled.iloc[:window_size+horizon], window_size, horizon, batch_size=1, shuffle=False)\n",
    "\n",
    "def _eps_tag(x: float) -> str:\n",
    "    return str(x).replace('.', '_')\n",
    "\n",
    "def log_cosh_metric(y_true, y_pred):\n",
    "    e = tf.cast(y_pred, tf.float32) - tf.cast(y_true, tf.float32)\n",
    "    # logcosh(x) = |x| + softplus(-2|x|) - log(2)  → estable y sin overflow\n",
    "    ae = tf.abs(e)\n",
    "    return tf.reduce_mean(ae + tf.nn.softplus(-2.0 * ae) - tf.math.log(2.0))\n",
    "log_cosh_metric.__name__ = \"log_cosh\"\n",
    "\n",
    "def make_huber_loss(delta: float):\n",
    "    base = tf.keras.losses.Huber(delta=float(delta))\n",
    "    def huber_loss(y_true, y_pred):\n",
    "        y_true = tf.cast(y_true, tf.float32); y_pred = tf.cast(y_pred, tf.float32)\n",
    "        return base(y_true, y_pred)\n",
    "    huber_loss.__name__ = f\"huber_delta_{_eps_tag(delta)}\"\n",
    "    return huber_loss\n",
    "\n",
    "def make_within_eps_vector_metric(eps_vec: np.ndarray, tag: str):\n",
    "    \"\"\"eps_vec shape (F,) en la MISMA escala que y_true/y_pred (normalizada).\"\"\"\n",
    "    eps_tf = tf.constant(eps_vec.astype(np.float32), dtype=tf.float32)  # (F,)\n",
    "    def within_eps(y_true, y_pred):\n",
    "        diff = tf.abs(tf.cast(y_pred, tf.float32) - tf.cast(y_true, tf.float32))  # (B,H,F)\n",
    "        thr  = eps_tf[tf.newaxis, tf.newaxis, :]                                   # (1,1,F)\n",
    "        hit  = tf.cast(diff <= thr, tf.float32)\n",
    "        return tf.reduce_mean(hit)\n",
    "    within_eps.__name__ = tag\n",
    "    return within_eps\n",
    "\n",
    "def build_within_metrics_minmax(scaler, eps_list: List[float], n_features: int):\n",
    "    \"\"\"\n",
    "    MinMaxScaler: eps = porcentaje * data_range_ por feature.\n",
    "    OJO: y_true/y_pred están normalizados, así que el umbral ya se pasa NORMALIZADO.\n",
    "    Con MinMax a [0,1], 'porcentaje del rango' == el propio porcentaje.\n",
    "    Para mantener la semántica, se usa directamente eps_list (0.5%, 1%, ... del rango).\n",
    "    \"\"\"\n",
    "    metrics = [log_cosh_metric]\n",
    "\n",
    "    # Verificación rápida de consistencia del scaler\n",
    "    if not hasattr(scaler, \"data_range_\"):\n",
    "        raise ValueError(\"Se esperaba un MinMaxScaler con atributo data_range_.\")\n",
    "    if len(scaler.data_range_) != n_features:\n",
    "        raise ValueError(\"scaler.data_range_ no coincide con n_features.\")\n",
    "\n",
    "    # En escala normalizada [0,1], el 'porcentaje del rango' es exactamente el valor eps.\n",
    "    # Por lo tanto, el vector de tolerancias NORMALIZADO es uniforme por cada feature (= eps).\n",
    "    for e in eps_list:\n",
    "        eps_vec = np.full((n_features,), float(e), dtype=np.float32)\n",
    "        tag = f\"within_eps_{_eps_tag(e)}\"\n",
    "        metrics.append(make_within_eps_vector_metric(eps_vec, tag))\n",
    "    return metrics\n",
    "\n",
    "def compute_autc_from_results(res: dict, eps_list: List[float]) -> float:\n",
    "    \"\"\"\n",
    "    Calcula AUTC normalizando por el rango [ε_min, ε_max], ∈ [0,1].\n",
    "    Toma los valores within_eps_* devueltos por model.evaluate(return_dict=True).\n",
    "    \"\"\"\n",
    "    eps = np.array(sorted(eps_list), dtype=np.float32)\n",
    "    acc = np.array([res.get(f\"within_eps_{str(e).replace('.','_')}\", np.nan) for e in eps],\n",
    "                   dtype=np.float32)\n",
    "\n",
    "    mask = np.isfinite(acc)\n",
    "    if mask.sum() < 2:    #Si faltan puntos o hay NaN, integra sobre los disponibles (requiere ≥ 2 puntos).\n",
    "        return float(\"nan\")\n",
    "\n",
    "    eps = eps[mask]\n",
    "    acc = acc[mask]\n",
    "    return float(np.trapz(acc, eps) / (eps[-1] - eps[0]))\n",
    "\n",
    "\n",
    "# === Compilar el modelo con las métricas within-ε correctas (escala normalizada) ===\n",
    "EPS_LIST = [0.005, 0.01, 0.02, 0.05, 0.1]\n",
    "n_features = len(feature_order)\n",
    "\n",
    "metrics = build_within_metrics_minmax(scaler, EPS_LIST, n_features=n_features)\n",
    "model_s.compile(optimizer=\"adam\", loss=make_huber_loss(delta_s), metrics=metrics)\n",
    "\n",
    "def evaluate_and_print(tag, dataset):\n",
    "    res = model_s.evaluate(dataset, return_dict=True, verbose=0)\n",
    "    print(f\"\\n=== Resultados {tag} (20→1) ===\")\n",
    "    print(f\"  Huber (loss):              {res.get('loss', float('nan')):.6f}\")\n",
    "    print(f\"  log_cosh:                  {res.get('log_cosh', float('nan')):.6f}\")\n",
    "    for e in EPS_LIST:\n",
    "        key = f\"within_eps_{str(e).replace('.','_')}\"\n",
    "        print(f\"  {key:26s}: {res.get(key, float('nan')):.6f}\")\n",
    "    autc = compute_autc_from_results(res, EPS_LIST)\n",
    "    print(f\"  AUTC[{min(EPS_LIST):.3f}–{max(EPS_LIST):.3f}]:         {autc:.6f}\")\n",
    "    return res\n",
    "\n",
    "# === Evaluación de S1, S2 y S3 (datasets completos) ===\n",
    "res1 = evaluate_and_print(\"S1\", ds1)\n",
    "res2 = evaluate_and_print(\"S2\", ds2)\n",
    "res3 = evaluate_and_print(\"S3\", ds3)\n",
    "\n",
    "# === Ventana ÚNICA (20→1) por cada S para inspección puntual y ver precios reales ===\n",
    "def single_window_arrays(df_scaled, w=20, h=1):\n",
    "    if len(df_scaled) < w + h:\n",
    "        raise ValueError(\"No hay suficientes días hábiles para 20→1.\")\n",
    "    X = df_scaled.iloc[0:w].to_numpy()[np.newaxis, :, :]              # (1,w,F)\n",
    "    Y = df_scaled.iloc[w:w+h].to_numpy()[np.newaxis, :, :]            # (1,h,F)\n",
    "    return X.astype(\"float32\"), Y.astype(\"float32\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8a1e0bfa-c9f0-4fd8-b1eb-673cc4b8de9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==== Tabla de métricas por escenario ====\n",
      "        loss  log_cosh      AUTC  within_eps_0_005  within_eps_0_01  \\\n",
      "S1  0.057957  2.930186  0.040000              0.04             0.04   \n",
      "S2  0.052172  2.599282  0.090526              0.00             0.00   \n",
      "S3  0.053469  2.663874  0.048421              0.00             0.00   \n",
      "\n",
      "    within_eps_0_02  within_eps_0_05  within_eps_0_1  \n",
      "S1             0.04             0.04            0.04  \n",
      "S2             0.04             0.12            0.12  \n",
      "S3             0.00             0.04            0.12  \n",
      "\n",
      "==== Medias (S1,S2,S3) ====\n",
      "loss                0.054533\n",
      "log_cosh            2.731114\n",
      "AUTC                0.059649\n",
      "within_eps_0_005    0.013333\n",
      "within_eps_0_01     0.013333\n",
      "within_eps_0_02     0.026667\n",
      "within_eps_0_05     0.066667\n",
      "within_eps_0_1      0.093333\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# --- 1) Construir tabla con todas las métricas por escenario ---\n",
    "def res_to_series(tag, res, eps_list):\n",
    "    d = {\n",
    "        \"loss\": res.get(\"loss\", np.nan),\n",
    "        \"log_cosh\": res.get(\"log_cosh\", np.nan),\n",
    "        \"AUTC\": compute_autc_from_results(res, eps_list),\n",
    "    }\n",
    "    for e in eps_list:\n",
    "        key = f\"within_eps_{str(e).replace('.','_')}\"\n",
    "        d[key] = res.get(key, np.nan)\n",
    "    s = pd.Series(d, name=tag)\n",
    "    return s\n",
    "\n",
    "s1 = res_to_series(\"S1\", res1, EPS_LIST)\n",
    "s2 = res_to_series(\"S2\", res2, EPS_LIST)\n",
    "s3 = res_to_series(\"S3\", res3, EPS_LIST)\n",
    "\n",
    "df_all = pd.concat([s1, s2, s3], axis=1).T  # filas: escenarios; cols: métricas\n",
    "df_mean = df_all.mean(axis=0)\n",
    "\n",
    "print(\"\\n==== Tabla de métricas por escenario ====\")\n",
    "print(df_all.round(6))\n",
    "print(\"\\n==== Medias (S1,S2,S3) ====\")\n",
    "print(df_mean.round(6))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30d680f5-0324-473b-a068-ad4c230b49eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 2) Gráfica A: curvas within-ε (ε en eje X, accuracy en eje Y) ---\n",
    "eps = np.array(EPS_LIST, dtype=float)\n",
    "\n",
    "def pick_from_df(df_row, eps_list):\n",
    "    vals = []\n",
    "    for e in eps_list:\n",
    "        key = f\"within_eps_{str(e).replace('.','_')}\"\n",
    "        vals.append(df_row[key])\n",
    "    return np.array(vals, dtype=float)\n",
    "\n",
    "w_s1 = pick_from_df(s1, EPS_LIST)\n",
    "w_s2 = pick_from_df(s2, EPS_LIST)\n",
    "w_s3 = pick_from_df(s3, EPS_LIST)\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(eps, w_s1, marker=\"o\", label=\"S1\")\n",
    "plt.plot(eps, w_s2, marker=\"o\", label=\"S2\")\n",
    "plt.plot(eps, w_s3, marker=\"o\", label=\"S3\")\n",
    "plt.title(\"Curva within-ε por escenario (20→1)\")\n",
    "plt.xlabel(\"ε (escala MinMax)\")\n",
    "plt.ylabel(\"Proporción de aciertos dentro de ε\")\n",
    "plt.grid(True, linestyle=\"--\", alpha=0.4)\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b46e90f-e262-4f13-934f-0c906c16167f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 3) Gráfica B: barras para loss, log_cosh y AUTC ---\n",
    "labels = [\"S1\",\"S2\",\"S3\"]\n",
    "loss_vals = [s1[\"loss\"], s2[\"loss\"], s3[\"loss\"]]\n",
    "lc_vals   = [s1[\"log_cosh\"], s2[\"log_cosh\"], s3[\"log_cosh\"]]\n",
    "autc_vals = [s1[\"AUTC\"], s2[\"AUTC\"], s3[\"AUTC\"]]\n",
    "\n",
    "# Barras: loss\n",
    "plt.figure()\n",
    "plt.bar(labels, loss_vals)\n",
    "plt.title(\"Huber loss por escenario\")\n",
    "plt.ylabel(\"loss\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Barras: log_cosh\n",
    "plt.figure()\n",
    "plt.bar(labels, lc_vals)\n",
    "plt.title(\"log_cosh por escenario\")\n",
    "plt.ylabel(\"log_cosh\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Barras: AUTC\n",
    "plt.figure()\n",
    "plt.bar(labels, autc_vals)\n",
    "plt.title(f\"AUTC[{min(EPS_LIST):.3f}–{max(EPS_LIST):.3f}] por escenario\")\n",
    "plt.ylabel(\"AUTC\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef9764f9-f141-43a6-bfd1-59858425b2ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 4) (Opcional) imprimir medias “bonitas” ya redondeadas ---\n",
    "print(\"\\n==== Medias (redondeadas) ====\")\n",
    "for k, v in df_mean.items():\n",
    "    print(f\"{k:>20s}: {v:.6f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84a2f2b1-aa11-4203-91a6-4a2f0216179f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
