{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "50209ab4-a448-4388-aa90-a437afe14b52",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-26 18:35:23.071710: I metal_plugin/src/device/metal_device.cc:1154] Metal device set to: Apple M2\n",
      "2025-09-26 18:35:23.071731: I metal_plugin/src/device/metal_device.cc:296] systemMemory: 16.00 GB\n",
      "2025-09-26 18:35:23.071737: I metal_plugin/src/device/metal_device.cc:313] maxCacheSize: 5.33 GB\n",
      "2025-09-26 18:35:23.071756: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:305] Could not identify NUMA node of platform GPU ID 0, defaulting to 0. Your kernel may not have been built with NUMA support.\n",
      "2025-09-26 18:35:23.071765: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:271] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 0 MB memory) -> physical PluggableDevice (device: 0, name: METAL, pci bus id: <undefined>)\n"
     ]
    }
   ],
   "source": [
    "# Carga de datos\n",
    "\n",
    "import pandas as pd\n",
    "df1 = pd.read_csv('./data/cierres_diarios_S1.csv', parse_dates=['Date'], index_col='Date')\n",
    "df2 = pd.read_csv('./data/cierres_diarios_S2.csv', parse_dates=['Date'], index_col='Date')\n",
    "df3 = pd.read_csv('./data/cierres_diarios_S3.csv', parse_dates=['Date'], index_col='Date')\n",
    "\n",
    "# Preparación de los datos\n",
    "\n",
    "# Eliminación de nulos\n",
    "df1.ffill(inplace=True) # rellena con el ultimo precio conocido\n",
    "df1.bfill(inplace=True) # por si hay alguna celda con NaN al principio\n",
    "df2.ffill(inplace=True) \n",
    "df2.bfill(inplace=True) \n",
    "df3.ffill(inplace=True) \n",
    "df3.bfill(inplace=True)\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from joblib import load\n",
    "from typing import List\n",
    "\n",
    "# Cargar scaler y columnas base (mismo orden que en entrenamiento)\n",
    "scaler = load(\"scaler_modelos.joblib\")\n",
    "\n",
    "\n",
    "# Como el scaler tiene 'feature_names_in_', usamos ese orden:\n",
    "feature_order = list(scaler.feature_names_in_)\n",
    "\n",
    "\n",
    "# Filtramos/ordenamos exactamente como en entrenamiento\n",
    "df1 = df1[feature_order].copy()\n",
    "df2 = df2[feature_order].copy()\n",
    "df3 = df3[feature_order].copy()\n",
    "\n",
    "\n",
    "# Escalar con el mismo scaler\n",
    "df1_scaled = pd.DataFrame(\n",
    "    scaler.transform(df1[feature_order]),\n",
    "    index=df1.index,\n",
    "    columns=feature_order\n",
    ").astype(\"float32\")\n",
    "\n",
    "df2_scaled = pd.DataFrame(\n",
    "    scaler.transform(df2[feature_order]),\n",
    "    index=df2.index,\n",
    "    columns=feature_order\n",
    ").astype(\"float32\")\n",
    "\n",
    "df3_scaled = pd.DataFrame(\n",
    "    scaler.transform(df3[feature_order]),\n",
    "    index=df3.index,\n",
    "    columns=feature_order\n",
    ").astype(\"float32\")\n",
    "\n",
    "# === Datasets coherentes con el entrenamiento: usar SIEMPRE datos ESCALADOS ===\n",
    "def make_dataset(data_scaled, window_size, horizon, batch_size=32, shuffle=False):\n",
    "    ds = tf.keras.preprocessing.timeseries_dataset_from_array(\n",
    "        data=data_scaled.values,\n",
    "        targets=None,\n",
    "        sequence_length=window_size + horizon,\n",
    "        sequence_stride=1,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=shuffle\n",
    "    )\n",
    "    return ds.map(\n",
    "        lambda seq: (\n",
    "            tf.cast(seq[:, :window_size, :], tf.float32),   # X\n",
    "            tf.cast(seq[:, window_size:, :], tf.float32)    # y\n",
    "        )\n",
    "    )\n",
    "\n",
    "window_size = 20\n",
    "horizon = 1\n",
    "\n",
    "# Evaluar SOLO el primer (20->1) del intervalo:\n",
    "ds1 = make_dataset(df1_scaled.iloc[:window_size+horizon], window_size, horizon, batch_size=1, shuffle=False)\n",
    "ds2 = make_dataset(df2_scaled.iloc[:window_size+horizon], window_size, horizon, batch_size=1, shuffle=False)\n",
    "ds3 = make_dataset(df3_scaled.iloc[:window_size+horizon], window_size, horizon, batch_size=1, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "52d503fd-9afd-48e1-9722-a27793181819",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Funciones comunes\n",
    "\n",
    "def _eps_tag(x: float) -> str:\n",
    "    return str(x).replace('.', '_')\n",
    "\n",
    "def log_cosh_metric(y_true, y_pred):\n",
    "    e = tf.cast(y_pred, tf.float32) - tf.cast(y_true, tf.float32)\n",
    "    # logcosh(x) = |x| + softplus(-2|x|) - log(2)  → estable y sin overflow\n",
    "    ae = tf.abs(e)\n",
    "    return tf.reduce_mean(ae + tf.nn.softplus(-2.0 * ae) - tf.math.log(2.0))\n",
    "log_cosh_metric.__name__ = \"log_cosh\"\n",
    "\n",
    "def make_huber_loss(delta: float):\n",
    "    base = tf.keras.losses.Huber(delta=float(delta))\n",
    "    def huber_loss(y_true, y_pred):\n",
    "        y_true = tf.cast(y_true, tf.float32); y_pred = tf.cast(y_pred, tf.float32)\n",
    "        return base(y_true, y_pred)\n",
    "    huber_loss.__name__ = f\"huber_delta_{_eps_tag(delta)}\"\n",
    "    return huber_loss\n",
    "\n",
    "def make_within_eps_vector_metric(eps_vec: np.ndarray, tag: str):\n",
    "    \"\"\"eps_vec shape (F,) en la MISMA escala que y_true/y_pred (normalizada).\"\"\"\n",
    "    eps_tf = tf.constant(eps_vec.astype(np.float32), dtype=tf.float32)  # (F,)\n",
    "    def within_eps(y_true, y_pred):\n",
    "        diff = tf.abs(tf.cast(y_pred, tf.float32) - tf.cast(y_true, tf.float32))  # (B,H,F)\n",
    "        thr  = eps_tf[tf.newaxis, tf.newaxis, :]                                   # (1,1,F)\n",
    "        hit  = tf.cast(diff <= thr, tf.float32)\n",
    "        return tf.reduce_mean(hit)\n",
    "    within_eps.__name__ = tag\n",
    "    return within_eps\n",
    "\n",
    "def build_within_metrics_minmax(scaler, eps_list: List[float], n_features: int):\n",
    "    \"\"\"\n",
    "    MinMaxScaler: eps = porcentaje * data_range_ por feature.\n",
    "    OJO: y_true/y_pred están normalizados, así que el umbral ya se pasa NORMALIZADO.\n",
    "    Con MinMax a [0,1], 'porcentaje del rango' == el propio porcentaje.\n",
    "    Para mantener la semántica, se usa directamente eps_list (0.5%, 1%, ... del rango).\n",
    "    \"\"\"\n",
    "    metrics = [log_cosh_metric]\n",
    "\n",
    "    # Verificación rápida de consistencia del scaler\n",
    "    if not hasattr(scaler, \"data_range_\"):\n",
    "        raise ValueError(\"Se esperaba un MinMaxScaler con atributo data_range_.\")\n",
    "    if len(scaler.data_range_) != n_features:\n",
    "        raise ValueError(\"scaler.data_range_ no coincide con n_features.\")\n",
    "\n",
    "    # En escala normalizada [0,1], el 'porcentaje del rango' es exactamente el valor eps.\n",
    "    # Por lo tanto, el vector de tolerancias NORMALIZADO es uniforme por cada feature (= eps).\n",
    "    for e in eps_list:\n",
    "        eps_vec = np.full((n_features,), float(e), dtype=np.float32)\n",
    "        tag = f\"within_eps_{_eps_tag(e)}\"\n",
    "        metrics.append(make_within_eps_vector_metric(eps_vec, tag))\n",
    "    return metrics\n",
    "\n",
    "def compute_autc_from_results(res: dict, eps_list: List[float]) -> float:\n",
    "    \"\"\"\n",
    "    Calcula AUTC normalizando por el rango [ε_min, ε_max], ∈ [0,1].\n",
    "    Toma los valores within_eps_* devueltos por model.evaluate(return_dict=True).\n",
    "    \"\"\"\n",
    "    eps = np.array(sorted(eps_list), dtype=np.float32)\n",
    "    acc = np.array([res.get(f\"within_eps_{str(e).replace('.','_')}\", np.nan) for e in eps],\n",
    "                   dtype=np.float32)\n",
    "\n",
    "    mask = np.isfinite(acc)\n",
    "    if mask.sum() < 2:    #Si faltan puntos o hay NaN, integra sobre los disponibles (requiere ≥ 2 puntos).\n",
    "        return float(\"nan\")\n",
    "\n",
    "    eps = eps[mask]\n",
    "    acc = acc[mask]\n",
    "    return float(np.trapz(acc, eps) / (eps[-1] - eps[0]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f88ed706-3759-4fd8-87b5-8583c32d8975",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Carga de modelos\n",
    "from tensorflow.keras.models import load_model \n",
    "\n",
    "# Carga del modelo GRU \n",
    "path_gru_s = \"./models_gru_huber_sweep/gru_huber_w20_h1_delta0_01735741273_S.keras\" \n",
    "model_gru_s = load_model(path_gru_s, compile=False) \n",
    "delta_gru_s = 0.017357412725687027\n",
    "\n",
    "# Carga del modelo CNN\n",
    "path_cnn_s = \"./models_cnn_huber_sweep/cnn_huber_w20_h1_delta0_04116208553_S.keras\"\n",
    "model_cnn_s = load_model(path_cnn_s, compile=False)\n",
    "delta_cnn_s = 0.04116208553314208\n",
    "\n",
    "# Carga del modelo Transformer\n",
    "path_trf_s = \"./models_transformer_huber_sweep/transformer_huber_w20_h1_delta0_04116208553_S.keras\"\n",
    "delta_trf_s = 0.04116208553314208\n",
    "\n",
    "# (Opcional) detectar capas personalizadas si existen en el entorno\n",
    "def maybe_get_custom_objects():\n",
    "    names = [\n",
    "        \"PositionalEmbedding\", \"Custom>PositionalEmbedding\",\n",
    "        \"TransformerBlock\", \"Custom>TransformerBlock\",\n",
    "        # añade aquí otros nombres si los usaste al registrar\n",
    "    ]\n",
    "    custom = {}\n",
    "    for n in names:\n",
    "        obj = globals().get(n, None)\n",
    "        if obj is not None:\n",
    "            custom[n] = obj\n",
    "    return custom if custom else None\n",
    "\n",
    "custom_objects = maybe_get_custom_objects()\n",
    "\n",
    "try:\n",
    "    if custom_objects:\n",
    "        model_trf_s = load_model(path_trf_s, compile=False, custom_objects=custom_objects)\n",
    "    else:\n",
    "        model_trf_s = load_model(path_trf_s, compile=False)\n",
    "except Exception as e:\n",
    "    raise RuntimeError(\n",
    "        \"No se pudo cargar el modelo Transformer. \"\n",
    "        \"Si tiene capas personalizadas, hay que pasarlas en custom_objects, por ejemplo:\\n\"\n",
    "        \"custom={'PositionalEmbedding': PositionalEmbedding, 'TransformerBlock': TransformerBlock}\\n\"\n",
    "        f\"Error original: {e}\"\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ef8a0468",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-26 18:35:27.273760: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:117] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Resultados S1 (20→1) GRU ===\n",
      "  Huber (loss):              0.057957\n",
      "  log_cosh:                  2.930186\n",
      "  within_eps_0_005          : 0.040000\n",
      "  within_eps_0_01           : 0.040000\n",
      "  within_eps_0_02           : 0.040000\n",
      "  within_eps_0_05           : 0.040000\n",
      "  within_eps_0_1            : 0.040000\n",
      "  AUTC[0.005–0.100]:         0.040000\n",
      "\n",
      "=== Resultados S2 (20→1) GRU ===\n",
      "  Huber (loss):              0.052172\n",
      "  log_cosh:                  2.599282\n",
      "  within_eps_0_005          : 0.000000\n",
      "  within_eps_0_01           : 0.000000\n",
      "  within_eps_0_02           : 0.040000\n",
      "  within_eps_0_05           : 0.120000\n",
      "  within_eps_0_1            : 0.120000\n",
      "  AUTC[0.005–0.100]:         0.090526\n",
      "\n",
      "=== Resultados S3 (20→1) GRU ===\n",
      "  Huber (loss):              0.053469\n",
      "  log_cosh:                  2.663874\n",
      "  within_eps_0_005          : 0.000000\n",
      "  within_eps_0_01           : 0.000000\n",
      "  within_eps_0_02           : 0.000000\n",
      "  within_eps_0_05           : 0.040000\n",
      "  within_eps_0_1            : 0.120000\n",
      "  AUTC[0.005–0.100]:         0.048421\n",
      "\n",
      "=== Resultados S1 (20→1) CNN ===\n",
      "  Huber (loss):              0.145861\n",
      "  log_cosh:                  3.090124\n",
      "  within_eps_0_005          : 0.000000\n",
      "  within_eps_0_01           : 0.000000\n",
      "  within_eps_0_02           : 0.000000\n",
      "  within_eps_0_05           : 0.000000\n",
      "  within_eps_0_1            : 0.000000\n",
      "  AUTC[0.005–0.100]:         0.000000\n",
      "\n",
      "=== Resultados S2 (20→1) CNN ===\n",
      "  Huber (loss):              0.128678\n",
      "  log_cosh:                  2.688421\n",
      "  within_eps_0_005          : 0.000000\n",
      "  within_eps_0_01           : 0.000000\n",
      "  within_eps_0_02           : 0.000000\n",
      "  within_eps_0_05           : 0.000000\n",
      "  within_eps_0_1            : 0.000000\n",
      "  AUTC[0.005–0.100]:         0.000000\n",
      "\n",
      "=== Resultados S3 (20→1) CNN ===\n",
      "  Huber (loss):              0.129464\n",
      "  log_cosh:                  2.709869\n",
      "  within_eps_0_005          : 0.040000\n",
      "  within_eps_0_01           : 0.040000\n",
      "  within_eps_0_02           : 0.040000\n",
      "  within_eps_0_05           : 0.040000\n",
      "  within_eps_0_1            : 0.080000\n",
      "  AUTC[0.005–0.100]:         0.050526\n",
      "\n",
      "=== Resultados S1 (20→1) TRF ===\n",
      "  Huber (loss):              0.137233\n",
      "  log_cosh:                  2.930740\n",
      "  within_eps_0_005          : 0.000000\n",
      "  within_eps_0_01           : 0.000000\n",
      "  within_eps_0_02           : 0.000000\n",
      "  within_eps_0_05           : 0.000000\n",
      "  within_eps_0_1            : 0.120000\n",
      "  AUTC[0.005–0.100]:         0.031579\n",
      "\n",
      "=== Resultados S2 (20→1) TRF ===\n",
      "  Huber (loss):              0.125692\n",
      "  log_cosh:                  2.626347\n",
      "  within_eps_0_005          : 0.000000\n",
      "  within_eps_0_01           : 0.000000\n",
      "  within_eps_0_02           : 0.000000\n",
      "  within_eps_0_05           : 0.000000\n",
      "  within_eps_0_1            : 0.080000\n",
      "  AUTC[0.005–0.100]:         0.021053\n",
      "\n",
      "=== Resultados S3 (20→1) TRF ===\n",
      "  Huber (loss):              0.130610\n",
      "  log_cosh:                  2.716467\n",
      "  within_eps_0_005          : 0.000000\n",
      "  within_eps_0_01           : 0.040000\n",
      "  within_eps_0_02           : 0.040000\n",
      "  within_eps_0_05           : 0.040000\n",
      "  within_eps_0_1            : 0.040000\n",
      "  AUTC[0.005–0.100]:         0.038947\n",
      "\n",
      "==== Tabla de métricas por escenario GRU ====\n",
      "        loss  log_cosh      AUTC  within_eps_0_005  within_eps_0_01  \\\n",
      "S1  0.057957  2.930186  0.040000              0.04             0.04   \n",
      "S2  0.052172  2.599282  0.090526              0.00             0.00   \n",
      "S3  0.053469  2.663874  0.048421              0.00             0.00   \n",
      "\n",
      "    within_eps_0_02  within_eps_0_05  within_eps_0_1  \n",
      "S1             0.04             0.04            0.04  \n",
      "S2             0.04             0.12            0.12  \n",
      "S3             0.00             0.04            0.12  \n",
      "\n",
      "==== Medias (S1,S2,S3) GRU ====\n",
      "loss                0.054533\n",
      "log_cosh            2.731114\n",
      "AUTC                0.059649\n",
      "within_eps_0_005    0.013333\n",
      "within_eps_0_01     0.013333\n",
      "within_eps_0_02     0.026667\n",
      "within_eps_0_05     0.066667\n",
      "within_eps_0_1      0.093333\n",
      "dtype: float64\n",
      "\n",
      "==== Tabla de métricas por escenario CNN ====\n",
      "        loss  log_cosh      AUTC  within_eps_0_005  within_eps_0_01  \\\n",
      "S1  0.145861  3.090124  0.000000              0.00             0.00   \n",
      "S2  0.128678  2.688421  0.000000              0.00             0.00   \n",
      "S3  0.129464  2.709869  0.050526              0.04             0.04   \n",
      "\n",
      "    within_eps_0_02  within_eps_0_05  within_eps_0_1  \n",
      "S1             0.00             0.00            0.00  \n",
      "S2             0.00             0.00            0.00  \n",
      "S3             0.04             0.04            0.08  \n",
      "\n",
      "==== Medias (S1,S2,S3) CNN ====\n",
      "loss                0.134668\n",
      "log_cosh            2.829472\n",
      "AUTC                0.016842\n",
      "within_eps_0_005    0.013333\n",
      "within_eps_0_01     0.013333\n",
      "within_eps_0_02     0.013333\n",
      "within_eps_0_05     0.013333\n",
      "within_eps_0_1      0.026667\n",
      "dtype: float64\n",
      "\n",
      "==== Tabla de métricas por escenario TRF ====\n",
      "        loss  log_cosh      AUTC  within_eps_0_005  within_eps_0_01  \\\n",
      "S1  0.137233  2.930740  0.031579               0.0             0.00   \n",
      "S2  0.125692  2.626347  0.021053               0.0             0.00   \n",
      "S3  0.130610  2.716467  0.038947               0.0             0.04   \n",
      "\n",
      "    within_eps_0_02  within_eps_0_05  within_eps_0_1  \n",
      "S1             0.00             0.00            0.12  \n",
      "S2             0.00             0.00            0.08  \n",
      "S3             0.04             0.04            0.04  \n",
      "\n",
      "==== Medias (S1,S2,S3) TRF ====\n",
      "loss                0.131178\n",
      "log_cosh            2.757851\n",
      "AUTC                0.030526\n",
      "within_eps_0_005    0.000000\n",
      "within_eps_0_01     0.013333\n",
      "within_eps_0_02     0.013333\n",
      "within_eps_0_05     0.013333\n",
      "within_eps_0_1      0.080000\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# === Compilar los modelos con las métricas within-ε correctas (escala normalizada) ===\n",
    "EPS_LIST = [0.005, 0.01, 0.02, 0.05, 0.1]\n",
    "n_features = len(feature_order)\n",
    "\n",
    "metrics = build_within_metrics_minmax(scaler, EPS_LIST, n_features=n_features)\n",
    "model_gru_s.compile(optimizer=\"adam\", loss=make_huber_loss(delta_gru_s), metrics=metrics)\n",
    "model_cnn_s.compile(optimizer=\"adam\", loss=make_huber_loss(delta_cnn_s), metrics=metrics)\n",
    "model_trf_s.compile(optimizer=\"adam\", loss=make_huber_loss(delta_trf_s), metrics=metrics)\n",
    "\n",
    "\n",
    "def evaluate_and_print(tag, dataset, model_s, name):\n",
    "    res = model_s.evaluate(dataset, return_dict=True, verbose=0)\n",
    "    print(f\"\\n=== Resultados {tag} (20→1) {name} ===\")\n",
    "    print(f\"  Huber (loss):              {res.get('loss', float('nan')):.6f}\")\n",
    "    print(f\"  log_cosh:                  {res.get('log_cosh', float('nan')):.6f}\")\n",
    "    for e in EPS_LIST:\n",
    "        key = f\"within_eps_{str(e).replace('.','_')}\"\n",
    "        print(f\"  {key:26s}: {res.get(key, float('nan')):.6f}\")\n",
    "    autc = compute_autc_from_results(res, EPS_LIST)\n",
    "    print(f\"  AUTC[{min(EPS_LIST):.3f}–{max(EPS_LIST):.3f}]:         {autc:.6f}\")\n",
    "    return res\n",
    "\n",
    "# === Evaluación de S1, S2 y S3 (datasets completos) ===\n",
    "res_gru_1 = evaluate_and_print(\"S1\", ds1, model_gru_s, \"GRU\")\n",
    "res_gru_2 = evaluate_and_print(\"S2\", ds2, model_gru_s, \"GRU\")\n",
    "res_gru_3 = evaluate_and_print(\"S3\", ds3, model_gru_s, \"GRU\")\n",
    "\n",
    "res_cnn_1 = evaluate_and_print(\"S1\", ds1, model_cnn_s, \"CNN\")\n",
    "res_cnn_2 = evaluate_and_print(\"S2\", ds2, model_cnn_s, \"CNN\")\n",
    "res_cnn_3 = evaluate_and_print(\"S3\", ds3, model_cnn_s, \"CNN\")\n",
    "\n",
    "res_trf_1 = evaluate_and_print(\"S1\", ds1, model_trf_s, \"TRF\")\n",
    "res_trf_2 = evaluate_and_print(\"S2\", ds2, model_trf_s, \"TRF\")\n",
    "res_trf_3 = evaluate_and_print(\"S3\", ds3, model_trf_s, \"TRF\")\n",
    "\n",
    "\n",
    "# === Ventana ÚNICA (20→1) por cada S para inspección puntual y ver precios reales ===\n",
    "def single_window_arrays(df_scaled, w=20, h=1):\n",
    "    if len(df_scaled) < w + h:\n",
    "        raise ValueError(\"No hay suficientes días hábiles para 20→1.\")\n",
    "    X = df_scaled.iloc[0:w].to_numpy()[np.newaxis, :, :]              # (1,w,F)\n",
    "    Y = df_scaled.iloc[w:w+h].to_numpy()[np.newaxis, :, :]            # (1,h,F)\n",
    "    return X.astype(\"float32\"), Y.astype(\"float32\")\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# --- 1) Construir tabla con todas las métricas por escenario ---\n",
    "def res_to_series(tag, res, eps_list):\n",
    "    d = {\n",
    "        \"loss\": res.get(\"loss\", np.nan),\n",
    "        \"log_cosh\": res.get(\"log_cosh\", np.nan),\n",
    "        \"AUTC\": compute_autc_from_results(res, eps_list),\n",
    "    }\n",
    "    for e in eps_list:\n",
    "        key = f\"within_eps_{str(e).replace('.','_')}\"\n",
    "        d[key] = res.get(key, np.nan)\n",
    "    s = pd.Series(d, name=tag)\n",
    "    return s\n",
    "\n",
    "s1_gru = res_to_series(\"S1\", res_gru_1, EPS_LIST)\n",
    "s2_gru = res_to_series(\"S2\", res_gru_2, EPS_LIST)\n",
    "s3_gru = res_to_series(\"S3\", res_gru_3, EPS_LIST)\n",
    "\n",
    "s1_cnn = res_to_series(\"S1\", res_cnn_1, EPS_LIST)\n",
    "s2_cnn = res_to_series(\"S2\", res_cnn_2, EPS_LIST)\n",
    "s3_cnn = res_to_series(\"S3\", res_cnn_3, EPS_LIST)\n",
    "\n",
    "s1_trf = res_to_series(\"S1\", res_trf_1, EPS_LIST)\n",
    "s2_trf = res_to_series(\"S2\", res_trf_2, EPS_LIST)\n",
    "s3_trf = res_to_series(\"S3\", res_trf_3, EPS_LIST)\n",
    "\n",
    "\n",
    "df_all_gru = pd.concat([s1_gru, s2_gru, s3_gru], axis=1).T  # filas: escenarios; cols: métricas\n",
    "df_mean_gru = df_all_gru.mean(axis=0)\n",
    "\n",
    "print(\"\\n==== Tabla de métricas por escenario GRU ====\")\n",
    "print(df_all_gru.round(6))\n",
    "print(\"\\n==== Medias (S1,S2,S3) GRU ====\")\n",
    "print(df_mean_gru.round(6))\n",
    "\n",
    "\n",
    "df_all_cnn = pd.concat([s1_cnn, s2_cnn, s3_cnn], axis=1).T  # filas: escenarios; cols: métricas\n",
    "df_mean_cnn = df_all_cnn.mean(axis=0)\n",
    "\n",
    "print(\"\\n==== Tabla de métricas por escenario CNN ====\")\n",
    "print(df_all_cnn.round(6))\n",
    "print(\"\\n==== Medias (S1,S2,S3) CNN ====\")\n",
    "print(df_mean_cnn.round(6))\n",
    "\n",
    "\n",
    "df_all_trf = pd.concat([s1_trf, s2_trf, s3_trf], axis=1).T  # filas: escenarios; cols: métricas\n",
    "df_mean_trf = df_all_trf.mean(axis=0)\n",
    "\n",
    "print(\"\\n==== Tabla de métricas por escenario TRF ====\")\n",
    "print(df_all_trf.round(6))\n",
    "print(\"\\n==== Medias (S1,S2,S3) TRF ====\")\n",
    "print(df_mean_trf.round(6))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ad504ce8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Resultados S1 (20→1) ENS(avg) ===\n",
      "  Huber (loss):              0.102204\n",
      "  log_cosh:                  2.761842\n",
      "  within_eps_0_005          : 0.000000\n",
      "  within_eps_0_01           : 0.040000\n",
      "  within_eps_0_02           : 0.040000\n",
      "  within_eps_0_05           : 0.080000\n",
      "  within_eps_0_1            : 0.120000\n",
      "  AUTC[0.005–0.100]:         0.076842\n",
      "\n",
      "=== Resultados S2 (20→1) ENS(avg) ===\n",
      "  Huber (loss):              0.091738\n",
      "  log_cosh:                  2.419807\n",
      "  within_eps_0_005          : 0.040000\n",
      "  within_eps_0_01           : 0.040000\n",
      "  within_eps_0_02           : 0.040000\n",
      "  within_eps_0_05           : 0.080000\n",
      "  within_eps_0_1            : 0.160000\n",
      "  AUTC[0.005–0.100]:         0.088421\n",
      "\n",
      "=== Resultados S3 (20→1) ENS(avg) ===\n",
      "  Huber (loss):              0.095087\n",
      "  log_cosh:                  2.479454\n",
      "  within_eps_0_005          : 0.000000\n",
      "  within_eps_0_01           : 0.000000\n",
      "  within_eps_0_02           : 0.000000\n",
      "  within_eps_0_05           : 0.000000\n",
      "  within_eps_0_1            : 0.040000\n",
      "  AUTC[0.005–0.100]:         0.010526\n",
      "\n",
      "==== Tabla de métricas por escenario ENS(avg) ====\n",
      "        loss  log_cosh      AUTC  within_eps_0_005  within_eps_0_01  \\\n",
      "S1  0.102204  2.761842  0.076842              0.00             0.04   \n",
      "S2  0.091738  2.419807  0.088421              0.04             0.04   \n",
      "S3  0.095087  2.479454  0.010526              0.00             0.00   \n",
      "\n",
      "    within_eps_0_02  within_eps_0_05  within_eps_0_1  \n",
      "S1             0.04             0.08            0.12  \n",
      "S2             0.04             0.08            0.16  \n",
      "S3             0.00             0.00            0.04  \n",
      "\n",
      "==== Medias (S1,S2,S3) ENS(avg) ====\n",
      "loss                0.096343\n",
      "log_cosh            2.553701\n",
      "AUTC                0.058596\n",
      "within_eps_0_005    0.013333\n",
      "within_eps_0_01     0.026667\n",
      "within_eps_0_02     0.026667\n",
      "within_eps_0_05     0.053333\n",
      "within_eps_0_1      0.106667\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# === Ensemble: promedio simple de (GRU + CNN + TRF) sobre S1, S2, S3 ===\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Average\n",
    "from tensorflow.keras import Input, Model\n",
    "\n",
    "# Delta del ensemble como media de los deltas individuales (sólo para el loss Huber de evaluación)\n",
    "delta_ens_s = float(np.mean([delta_gru_s, delta_cnn_s, delta_trf_s]))\n",
    "\n",
    "# Construcción del modelo ensemble por promediado simple (mismo input que los modelos base)\n",
    "inp = Input(shape=model_gru_s.input_shape[1:], name=\"ensemble_input\")\n",
    "y_gru = model_gru_s(inp, training=False)\n",
    "y_cnn = model_cnn_s(inp, training=False)\n",
    "y_trf = model_trf_s(inp, training=False)\n",
    "y_avg = Average(name=\"avg_logits\")([y_gru, y_cnn, y_trf])\n",
    "model_ens_s = Model(inp, y_avg, name=\"ensemble_avg_gru_cnn_trf\")\n",
    "\n",
    "# Compilación con las mismas métricas within-ε y log_cosh que los otros modelos\n",
    "# (reutilizamos 'metrics' y EPS_LIST ya definidas arriba)\n",
    "model_ens_s.compile(optimizer=\"adam\", loss=make_huber_loss(delta_ens_s), metrics=metrics)\n",
    "\n",
    "# Evaluación y salida detallada en el mismo formato\n",
    "res_ens_1 = evaluate_and_print(\"S1\", ds1, model_ens_s, \"ENS(avg)\")\n",
    "res_ens_2 = evaluate_and_print(\"S2\", ds2, model_ens_s, \"ENS(avg)\")\n",
    "res_ens_3 = evaluate_and_print(\"S3\", ds3, model_ens_s, \"ENS(avg)\")\n",
    "\n",
    "# Conversión de resultados a Series, con AUTC y within-ε, igual que en GRU/CNN/TRF\n",
    "s1_ens = res_to_series(\"S1\", res_ens_1, EPS_LIST)\n",
    "s2_ens = res_to_series(\"S2\", res_ens_2, EPS_LIST)\n",
    "s3_ens = res_to_series(\"S3\", res_ens_3, EPS_LIST)\n",
    "\n",
    "# DataFrame (filas: escenarios; columnas: métricas)\n",
    "df_all_ens = pd.concat([s1_ens, s2_ens, s3_ens], axis=1).T\n",
    "df_mean_ens = df_all_ens.mean(axis=0)\n",
    "\n",
    "print(\"\\n==== Tabla de métricas por escenario ENS(avg) ====\")\n",
    "print(df_all_ens.round(6))\n",
    "print(\"\\n==== Medias (S1,S2,S3) ENS(avg) ====\")\n",
    "print(df_mean_ens.round(6))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c82c778d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "== Pesos ENS(w=AUTC) [GRU, CNN, TRF] ==> [0.557377 0.157377 0.285246]\n",
      "== Pesos ENS(w=1/log_cosh) [GRU, CNN, TRF] ==> [0.338347 0.326586 0.335067]\n",
      "WARNING:tensorflow:5 out of the last 13 calls to <function TensorFlowTrainer._make_function.<locals>.multi_step_on_iterator at 0x309f63250> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "\n",
      "=== Resultados S1 (20→1) ENS(w=AUTC) ===\n",
      "  Huber (loss):              0.103462\n",
      "  log_cosh:                  2.787976\n",
      "  within_eps_0_005          : 0.040000\n",
      "  within_eps_0_01           : 0.040000\n",
      "  within_eps_0_02           : 0.040000\n",
      "  within_eps_0_05           : 0.120000\n",
      "  within_eps_0_1            : 0.160000\n",
      "  AUTC[0.005–0.100]:         0.105263\n",
      "\n",
      "=== Resultados S2 (20→1) ENS(w=AUTC) ===\n",
      "  Huber (loss):              0.093154\n",
      "  log_cosh:                  2.458894\n",
      "  within_eps_0_005          : 0.000000\n",
      "  within_eps_0_01           : 0.000000\n",
      "  within_eps_0_02           : 0.040000\n",
      "  within_eps_0_05           : 0.080000\n",
      "  within_eps_0_1            : 0.120000\n",
      "  AUTC[0.005–0.100]:         0.073684\n",
      "\n",
      "=== Resultados S3 (20→1) ENS(w=AUTC) ===\n",
      "  Huber (loss):              0.096448\n",
      "  log_cosh:                  2.527182\n",
      "  within_eps_0_005          : 0.040000\n",
      "  within_eps_0_01           : 0.040000\n",
      "  within_eps_0_02           : 0.080000\n",
      "  within_eps_0_05           : 0.080000\n",
      "  within_eps_0_1            : 0.080000\n",
      "  AUTC[0.005–0.100]:         0.075789\n",
      "\n",
      "==== Tabla de métricas por escenario ENS(w=AUTC) ====\n",
      "        loss  log_cosh      AUTC  within_eps_0_005  within_eps_0_01  \\\n",
      "S1  0.103462  2.787976  0.105263              0.04             0.04   \n",
      "S2  0.093154  2.458894  0.073684              0.00             0.00   \n",
      "S3  0.096448  2.527182  0.075789              0.04             0.04   \n",
      "\n",
      "    within_eps_0_02  within_eps_0_05  within_eps_0_1  \n",
      "S1             0.04             0.12            0.16  \n",
      "S2             0.04             0.08            0.12  \n",
      "S3             0.08             0.08            0.08  \n",
      "\n",
      "==== Medias (S1,S2,S3) ENS(w=AUTC) ====\n",
      "loss                0.097688\n",
      "log_cosh            2.591351\n",
      "AUTC                0.084912\n",
      "within_eps_0_005    0.026667\n",
      "within_eps_0_01     0.026667\n",
      "within_eps_0_02     0.053333\n",
      "within_eps_0_05     0.093333\n",
      "within_eps_0_1      0.120000\n",
      "dtype: float64\n",
      "WARNING:tensorflow:5 out of the last 13 calls to <function TensorFlowTrainer._make_function.<locals>.multi_step_on_iterator at 0x30e8c4310> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "\n",
      "=== Resultados S1 (20→1) ENS(w=1/log_cosh) ===\n",
      "  Huber (loss):              0.102198\n",
      "  log_cosh:                  2.761524\n",
      "  within_eps_0_005          : 0.000000\n",
      "  within_eps_0_01           : 0.040000\n",
      "  within_eps_0_02           : 0.080000\n",
      "  within_eps_0_05           : 0.080000\n",
      "  within_eps_0_1            : 0.160000\n",
      "  AUTC[0.005–0.100]:         0.095789\n",
      "\n",
      "=== Resultados S2 (20→1) ENS(w=1/log_cosh) ===\n",
      "  Huber (loss):              0.091761\n",
      "  log_cosh:                  2.420146\n",
      "  within_eps_0_005          : 0.000000\n",
      "  within_eps_0_01           : 0.000000\n",
      "  within_eps_0_02           : 0.040000\n",
      "  within_eps_0_05           : 0.080000\n",
      "  within_eps_0_1            : 0.160000\n",
      "  AUTC[0.005–0.100]:         0.084211\n",
      "\n",
      "=== Resultados S3 (20→1) ENS(w=1/log_cosh) ===\n",
      "  Huber (loss):              0.095129\n",
      "  log_cosh:                  2.480255\n",
      "  within_eps_0_005          : 0.000000\n",
      "  within_eps_0_01           : 0.000000\n",
      "  within_eps_0_02           : 0.000000\n",
      "  within_eps_0_05           : 0.000000\n",
      "  within_eps_0_1            : 0.040000\n",
      "  AUTC[0.005–0.100]:         0.010526\n",
      "\n",
      "==== Tabla de métricas por escenario ENS(w=1/log_cosh) ====\n",
      "        loss  log_cosh      AUTC  within_eps_0_005  within_eps_0_01  \\\n",
      "S1  0.102198  2.761524  0.095789               0.0             0.04   \n",
      "S2  0.091761  2.420146  0.084211               0.0             0.00   \n",
      "S3  0.095129  2.480255  0.010526               0.0             0.00   \n",
      "\n",
      "    within_eps_0_02  within_eps_0_05  within_eps_0_1  \n",
      "S1             0.08             0.08            0.16  \n",
      "S2             0.04             0.08            0.16  \n",
      "S3             0.00             0.00            0.04  \n",
      "\n",
      "==== Medias (S1,S2,S3) ENS(w=1/log_cosh) ====\n",
      "loss                0.096363\n",
      "log_cosh            2.553975\n",
      "AUTC                0.063509\n",
      "within_eps_0_005    0.000000\n",
      "within_eps_0_01     0.013333\n",
      "within_eps_0_02     0.040000\n",
      "within_eps_0_05     0.053333\n",
      "within_eps_0_1      0.120000\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# === Ensembles ponderados (GRU + CNN + TRF) usando métricas AUTC y log_cosh ===\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import Input, Model\n",
    "from tensorflow.keras.layers import Lambda\n",
    "\n",
    "# 1) Cálculo de pesos a partir de las métricas agregadas (medias S1,S2,S3) de cada modelo\n",
    "#    - AUTC: mayor es mejor → pesos ∝ AUTC\n",
    "#    - log_cosh: menor es mejor → pesos ∝ 1 / log_cosh\n",
    "def _safe_array(vals):\n",
    "    vals = np.array(vals, dtype=np.float64)\n",
    "    if not np.all(np.isfinite(vals)):\n",
    "        # Si hay NaN/inf, reemplazar por 0 y luego renormalizar\n",
    "        vals = np.where(np.isfinite(vals), vals, 0.0)\n",
    "    return vals\n",
    "\n",
    "def _normalize(w):\n",
    "    s = np.sum(w)\n",
    "    if s <= 0 or not np.isfinite(s):\n",
    "        # fallback a pesos iguales\n",
    "        return np.array([1/3, 1/3, 1/3], dtype=np.float64)\n",
    "    return (w / s).astype(np.float64)\n",
    "\n",
    "# Extraer métricas promedio de cada modelo (ya calculadas arriba)\n",
    "autc_gru = float(df_mean_gru.get(\"AUTC\", np.nan))\n",
    "autc_cnn = float(df_mean_cnn.get(\"AUTC\", np.nan))\n",
    "autc_trf = float(df_mean_trf.get(\"AUTC\", np.nan))\n",
    "\n",
    "lcsh_gru = float(df_mean_gru.get(\"log_cosh\", np.nan))\n",
    "lcsh_cnn = float(df_mean_cnn.get(\"log_cosh\", np.nan))\n",
    "lcsh_trf = float(df_mean_trf.get(\"log_cosh\", np.nan))\n",
    "\n",
    "# Pesos por AUTC\n",
    "w_autc_raw = _safe_array([autc_gru, autc_cnn, autc_trf])\n",
    "w_autc = _normalize(w_autc_raw)\n",
    "\n",
    "# Pesos por log_cosh inverso (añadimos epsilon para evitar división por 0)\n",
    "_eps = 1e-12\n",
    "w_lcsh_raw = _safe_array([1.0 / max(lcsh_gru, _eps),\n",
    "                          1.0 / max(lcsh_cnn, _eps),\n",
    "                          1.0 / max(lcsh_trf, _eps)])\n",
    "w_lcsh = _normalize(w_lcsh_raw)\n",
    "\n",
    "print(\"\\n== Pesos ENS(w=AUTC) [GRU, CNN, TRF] ==>\", np.round(w_autc, 6))\n",
    "print(\"== Pesos ENS(w=1/log_cosh) [GRU, CNN, TRF] ==>\", np.round(w_lcsh, 6))\n",
    "\n",
    "# 2) Construcción de los dos ensembles ponderados\n",
    "inp_w = Input(shape=model_gru_s.input_shape[1:], name=\"ensemble_w_input\")\n",
    "\n",
    "y_gru_w = model_gru_s(inp_w, training=False)\n",
    "y_cnn_w = model_cnn_s(inp_w, training=False)\n",
    "y_trf_w = model_trf_s(inp_w, training=False)\n",
    "\n",
    "w0a, w1a, w2a = [float(x) for x in w_autc.tolist()]\n",
    "w0l, w1l, w2l = [float(x) for x in w_lcsh.tolist()]\n",
    "\n",
    "y_wavg_autc = Lambda(lambda t: t[0]*w0a + t[1]*w1a + t[2]*w2a, name=\"weighted_avg_autc\")([y_gru_w, y_cnn_w, y_trf_w])\n",
    "y_wavg_lcsh = Lambda(lambda t: t[0]*w0l + t[1]*w1l + t[2]*w2l, name=\"weighted_avg_lcsh\")([y_gru_w, y_cnn_w, y_trf_w])\n",
    "\n",
    "model_ens_autc_s = Model(inp_w, y_wavg_autc, name=\"ensemble_w_autc_gru_cnn_trf\")\n",
    "model_ens_lcsh_s = Model(inp_w, y_wavg_lcsh, name=\"ensemble_w_lcsh_gru_cnn_trf\")\n",
    "\n",
    "# 3) Compilación con mismas métricas; loss Huber con delta medio\n",
    "delta_ens_s = float(np.mean([delta_gru_s, delta_cnn_s, delta_trf_s]))\n",
    "model_ens_autc_s.compile(optimizer=\"adam\", loss=make_huber_loss(delta_ens_s), metrics=metrics)\n",
    "model_ens_lcsh_s.compile(optimizer=\"adam\", loss=make_huber_loss(delta_ens_s), metrics=metrics)\n",
    "\n",
    "# 4) Evaluación (S1, S2, S3) con el mismo formato\n",
    "# ENS(w=AUTC)\n",
    "res_ensa1 = evaluate_and_print(\"S1\", ds1, model_ens_autc_s, \"ENS(w=AUTC)\")\n",
    "res_ensa2 = evaluate_and_print(\"S2\", ds2, model_ens_autc_s, \"ENS(w=AUTC)\")\n",
    "res_ensa3 = evaluate_and_print(\"S3\", ds3, model_ens_autc_s, \"ENS(w=AUTC)\")\n",
    "\n",
    "s1_ensa = res_to_series(\"S1\", res_ensa1, EPS_LIST)\n",
    "s2_ensa = res_to_series(\"S2\", res_ensa2, EPS_LIST)\n",
    "s3_ensa = res_to_series(\"S3\", res_ensa3, EPS_LIST)\n",
    "\n",
    "df_all_ensa = pd.concat([s1_ensa, s2_ensa, s3_ensa], axis=1).T\n",
    "df_mean_ensa = df_all_ensa.mean(axis=0)\n",
    "\n",
    "print(\"\\n==== Tabla de métricas por escenario ENS(w=AUTC) ====\")\n",
    "print(df_all_ensa.round(6))\n",
    "print(\"\\n==== Medias (S1,S2,S3) ENS(w=AUTC) ====\")\n",
    "print(df_mean_ensa.round(6))\n",
    "\n",
    "# ENS(w=1/log_cosh)\n",
    "res_ensl1 = evaluate_and_print(\"S1\", ds1, model_ens_lcsh_s, \"ENS(w=1/log_cosh)\")\n",
    "res_ensl2 = evaluate_and_print(\"S2\", ds2, model_ens_lcsh_s, \"ENS(w=1/log_cosh)\")\n",
    "res_ensl3 = evaluate_and_print(\"S3\", ds3, model_ens_lcsh_s, \"ENS(w=1/log_cosh)\")\n",
    "\n",
    "s1_ensl = res_to_series(\"S1\", res_ensl1, EPS_LIST)\n",
    "s2_ensl = res_to_series(\"S2\", res_ensl2, EPS_LIST)\n",
    "s3_ensl = res_to_series(\"S3\", res_ensl3, EPS_LIST)\n",
    "\n",
    "df_all_ensl = pd.concat([s1_ensl, s2_ensl, s3_ensl], axis=1).T\n",
    "df_mean_ensl = df_all_ensl.mean(axis=0)\n",
    "\n",
    "print(\"\\n==== Tabla de métricas por escenario ENS(w=1/log_cosh) ====\")\n",
    "print(df_all_ensl.round(6))\n",
    "print(\"\\n==== Medias (S1,S2,S3) ENS(w=1/log_cosh) ====\")\n",
    "print(df_mean_ensl.round(6))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3f871eba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "== Pesos mixtos [GRU, CNN, TRF] ==\n",
      "  25% AUTC + 75% 1/log_cosh -> [0.393105 0.284284 0.322612]\n",
      "  50% AUTC + 50% 1/log_cosh -> [0.447862 0.241981 0.310156]\n",
      "  75% AUTC + 25% 1/log_cosh -> [0.50262  0.199679 0.297701]\n",
      "\n",
      "=== Resultados S1 (20→1) ENS(25%AUTC+75%1/log_cosh) ===\n",
      "  Huber (loss):              0.102324\n",
      "  log_cosh:                  2.761169\n",
      "  within_eps_0_005          : 0.000000\n",
      "  within_eps_0_01           : 0.000000\n",
      "  within_eps_0_02           : 0.040000\n",
      "  within_eps_0_05           : 0.120000\n",
      "  within_eps_0_1            : 0.120000\n",
      "  AUTC[0.005–0.100]:         0.090526\n",
      "\n",
      "=== Resultados S2 (20→1) ENS(25%AUTC+75%1/log_cosh) ===\n",
      "  Huber (loss):              0.091928\n",
      "  log_cosh:                  2.423157\n",
      "  within_eps_0_005          : 0.000000\n",
      "  within_eps_0_01           : 0.000000\n",
      "  within_eps_0_02           : 0.000000\n",
      "  within_eps_0_05           : 0.080000\n",
      "  within_eps_0_1            : 0.160000\n",
      "  AUTC[0.005–0.100]:         0.075789\n",
      "\n",
      "=== Resultados S3 (20→1) ENS(25%AUTC+75%1/log_cosh) ===\n",
      "  Huber (loss):              0.095310\n",
      "  log_cosh:                  2.485475\n",
      "  within_eps_0_005          : 0.000000\n",
      "  within_eps_0_01           : 0.000000\n",
      "  within_eps_0_02           : 0.000000\n",
      "  within_eps_0_05           : 0.000000\n",
      "  within_eps_0_1            : 0.040000\n",
      "  AUTC[0.005–0.100]:         0.010526\n",
      "\n",
      "==== Tabla de métricas por escenario ENS(25%AUTC+75%1/log_cosh) ====\n",
      "        loss  log_cosh      AUTC  within_eps_0_005  within_eps_0_01  \\\n",
      "S1  0.102324  2.761169  0.090526               0.0              0.0   \n",
      "S2  0.091928  2.423157  0.075789               0.0              0.0   \n",
      "S3  0.095310  2.485475  0.010526               0.0              0.0   \n",
      "\n",
      "    within_eps_0_02  within_eps_0_05  within_eps_0_1  \n",
      "S1             0.04             0.12            0.12  \n",
      "S2             0.00             0.08            0.16  \n",
      "S3             0.00             0.00            0.04  \n",
      "\n",
      "==== Medias (S1,S2,S3) ENS(25%AUTC+75%1/log_cosh) ====\n",
      "loss                0.096521\n",
      "log_cosh            2.556600\n",
      "AUTC                0.058947\n",
      "within_eps_0_005    0.000000\n",
      "within_eps_0_01     0.000000\n",
      "within_eps_0_02     0.013333\n",
      "within_eps_0_05     0.066667\n",
      "within_eps_0_1      0.106667\n",
      "dtype: float64\n",
      "\n",
      "=== Resultados S1 (20→1) ENS(50%AUTC+50%1/log_cosh) ===\n",
      "  Huber (loss):              0.102613\n",
      "  log_cosh:                  2.765402\n",
      "  within_eps_0_005          : 0.000000\n",
      "  within_eps_0_01           : 0.000000\n",
      "  within_eps_0_02           : 0.040000\n",
      "  within_eps_0_05           : 0.120000\n",
      "  within_eps_0_1            : 0.120000\n",
      "  AUTC[0.005–0.100]:         0.090526\n",
      "\n",
      "=== Resultados S2 (20→1) ENS(50%AUTC+50%1/log_cosh) ===\n",
      "  Huber (loss):              0.092128\n",
      "  log_cosh:                  2.430580\n",
      "  within_eps_0_005          : 0.040000\n",
      "  within_eps_0_01           : 0.040000\n",
      "  within_eps_0_02           : 0.080000\n",
      "  within_eps_0_05           : 0.120000\n",
      "  within_eps_0_1            : 0.160000\n",
      "  AUTC[0.005–0.100]:         0.113684\n",
      "\n",
      "=== Resultados S3 (20→1) ENS(50%AUTC+50%1/log_cosh) ===\n",
      "  Huber (loss):              0.095491\n",
      "  log_cosh:                  2.495038\n",
      "  within_eps_0_005          : 0.000000\n",
      "  within_eps_0_01           : 0.000000\n",
      "  within_eps_0_02           : 0.000000\n",
      "  within_eps_0_05           : 0.040000\n",
      "  within_eps_0_1            : 0.040000\n",
      "  AUTC[0.005–0.100]:         0.027368\n",
      "\n",
      "==== Tabla de métricas por escenario ENS(50%AUTC+50%1/log_cosh) ====\n",
      "        loss  log_cosh      AUTC  within_eps_0_005  within_eps_0_01  \\\n",
      "S1  0.102613  2.765402  0.090526              0.00             0.00   \n",
      "S2  0.092128  2.430580  0.113684              0.04             0.04   \n",
      "S3  0.095491  2.495038  0.027368              0.00             0.00   \n",
      "\n",
      "    within_eps_0_02  within_eps_0_05  within_eps_0_1  \n",
      "S1             0.04             0.12            0.12  \n",
      "S2             0.08             0.12            0.16  \n",
      "S3             0.00             0.04            0.04  \n",
      "\n",
      "==== Medias (S1,S2,S3) ENS(50%AUTC+50%1/log_cosh) ====\n",
      "loss                0.096744\n",
      "log_cosh            2.563673\n",
      "AUTC                0.077193\n",
      "within_eps_0_005    0.013333\n",
      "within_eps_0_01     0.013333\n",
      "within_eps_0_02     0.040000\n",
      "within_eps_0_05     0.093333\n",
      "within_eps_0_1      0.106667\n",
      "dtype: float64\n",
      "\n",
      "=== Resultados S1 (20→1) ENS(75%AUTC+25%1/log_cosh) ===\n",
      "  Huber (loss):              0.102976\n",
      "  log_cosh:                  2.774330\n",
      "  within_eps_0_005          : 0.000000\n",
      "  within_eps_0_01           : 0.000000\n",
      "  within_eps_0_02           : 0.040000\n",
      "  within_eps_0_05           : 0.080000\n",
      "  within_eps_0_1            : 0.080000\n",
      "  AUTC[0.005–0.100]:         0.063158\n",
      "\n",
      "=== Resultados S2 (20→1) ENS(75%AUTC+25%1/log_cosh) ===\n",
      "  Huber (loss):              0.092482\n",
      "  log_cosh:                  2.442494\n",
      "  within_eps_0_005          : 0.000000\n",
      "  within_eps_0_01           : 0.040000\n",
      "  within_eps_0_02           : 0.040000\n",
      "  within_eps_0_05           : 0.080000\n",
      "  within_eps_0_1            : 0.200000\n",
      "  AUTC[0.005–0.100]:         0.097895\n",
      "\n",
      "=== Resultados S3 (20→1) ENS(75%AUTC+25%1/log_cosh) ===\n",
      "  Huber (loss):              0.095769\n",
      "  log_cosh:                  2.508962\n",
      "  within_eps_0_005          : 0.000000\n",
      "  within_eps_0_01           : 0.000000\n",
      "  within_eps_0_02           : 0.040000\n",
      "  within_eps_0_05           : 0.080000\n",
      "  within_eps_0_1            : 0.120000\n",
      "  AUTC[0.005–0.100]:         0.073684\n",
      "\n",
      "==== Tabla de métricas por escenario ENS(75%AUTC+25%1/log_cosh) ====\n",
      "        loss  log_cosh      AUTC  within_eps_0_005  within_eps_0_01  \\\n",
      "S1  0.102976  2.774330  0.063158               0.0             0.00   \n",
      "S2  0.092482  2.442494  0.097895               0.0             0.04   \n",
      "S3  0.095769  2.508962  0.073684               0.0             0.00   \n",
      "\n",
      "    within_eps_0_02  within_eps_0_05  within_eps_0_1  \n",
      "S1             0.04             0.08            0.08  \n",
      "S2             0.04             0.08            0.20  \n",
      "S3             0.04             0.08            0.12  \n",
      "\n",
      "==== Medias (S1,S2,S3) ENS(75%AUTC+25%1/log_cosh) ====\n",
      "loss                0.097076\n",
      "log_cosh            2.575262\n",
      "AUTC                0.078246\n",
      "within_eps_0_005    0.000000\n",
      "within_eps_0_01     0.013333\n",
      "within_eps_0_02     0.040000\n",
      "within_eps_0_05     0.080000\n",
      "within_eps_0_1      0.133333\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# === Ensembles ponderados mixtos: AUTC vs 1/log_cosh (25/75, 50/50, 75/25) ===\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import Input, Model\n",
    "from tensorflow.keras.layers import Lambda\n",
    "\n",
    "# Asegurar que existen w_autc y w_lcsh; si no, recomputar desde df_mean_* (medias S1,S2,S3)\n",
    "def _safe_array(vals):\n",
    "    vals = np.array(vals, dtype=np.float64)\n",
    "    vals = np.where(np.isfinite(vals), vals, 0.0)\n",
    "    return vals\n",
    "\n",
    "def _normalize(w):\n",
    "    s = float(np.sum(w))\n",
    "    if not np.isfinite(s) or s <= 0:\n",
    "        return np.array([1/3, 1/3, 1/3], dtype=np.float64)\n",
    "    return (w / s).astype(np.float64)\n",
    "\n",
    "try:\n",
    "    w_autc, w_lcsh  # noqa: F401\n",
    "except NameError:\n",
    "    autc_gru = float(df_mean_gru.get(\"AUTC\", np.nan))\n",
    "    autc_cnn = float(df_mean_cnn.get(\"AUTC\", np.nan))\n",
    "    autc_trf = float(df_mean_trf.get(\"AUTC\", np.nan))\n",
    "    lcsh_gru = float(df_mean_gru.get(\"log_cosh\", np.nan))\n",
    "    lcsh_cnn = float(df_mean_cnn.get(\"log_cosh\", np.nan))\n",
    "    lcsh_trf = float(df_mean_trf.get(\"log_cosh\", np.nan))\n",
    "\n",
    "    w_autc_raw = _safe_array([autc_gru, autc_cnn, autc_trf])\n",
    "    w_autc = _normalize(w_autc_raw)\n",
    "\n",
    "    _eps = 1e-12\n",
    "    w_lcsh_raw = _safe_array([1.0 / max(lcsh_gru, _eps),\n",
    "                              1.0 / max(lcsh_cnn, _eps),\n",
    "                              1.0 / max(lcsh_trf, _eps)])\n",
    "    w_lcsh = _normalize(w_lcsh_raw)\n",
    "\n",
    "# Mezclas: alpha para AUTC, (1-alpha) para 1/log_cosh\n",
    "alphas = [0.25, 0.50, 0.75]\n",
    "w_mixes = []\n",
    "for a in alphas:\n",
    "    w_mix = _normalize(a * w_autc + (1.0 - a) * w_lcsh)\n",
    "    w_mixes.append(w_mix)\n",
    "\n",
    "print(\"\\n== Pesos mixtos [GRU, CNN, TRF] ==\")\n",
    "for a, w_mix in zip(alphas, w_mixes):\n",
    "    print(f\"  {int(a*100)}% AUTC + {int((1-a)*100)}% 1/log_cosh ->\", np.round(w_mix, 6))\n",
    "\n",
    "# Construcción de los tres ensembles y evaluación\n",
    "delta_ens_s = float(np.mean([delta_gru_s, delta_cnn_s, delta_trf_s]))\n",
    "\n",
    "def build_weighted_ensemble(weights, name_suffix):\n",
    "    inp = Input(shape=model_gru_s.input_shape[1:], name=f\"ens_mix_input_{name_suffix}\")\n",
    "    y0 = model_gru_s(inp, training=False)\n",
    "    y1 = model_cnn_s(inp, training=False)\n",
    "    y2 = model_trf_s(inp, training=False)\n",
    "    w0, w1, w2 = [float(x) for x in weights.tolist()]\n",
    "    y = Lambda(lambda t: t[0]*w0 + t[1]*w1 + t[2]*w2, name=f\"weighted_avg_{name_suffix}\")([y0, y1, y2])\n",
    "    m = Model(inp, y, name=f\"ensemble_mix_{name_suffix}\")\n",
    "    m.compile(optimizer=\"adam\", loss=make_huber_loss(delta_ens_s), metrics=metrics)\n",
    "    return m\n",
    "\n",
    "models_mix = [\n",
    "    (\"25_75\", w_mixes[0], \"ENS(25%AUTC+75%1/log_cosh)\"),\n",
    "    (\"50_50\", w_mixes[1], \"ENS(50%AUTC+50%1/log_cosh)\"),\n",
    "    (\"75_25\", w_mixes[2], \"ENS(75%AUTC+25%1/log_cosh)\"),\n",
    "]\n",
    "\n",
    "# Evaluación y tablas\n",
    "for tag_mix, w_mix, pretty_name in models_mix:\n",
    "    model_mix = build_weighted_ensemble(w_mix, tag_mix)\n",
    "    r1 = evaluate_and_print(\"S1\", ds1, model_mix, pretty_name)\n",
    "    r2 = evaluate_and_print(\"S2\", ds2, model_mix, pretty_name)\n",
    "    r3 = evaluate_and_print(\"S3\", ds3, model_mix, pretty_name)\n",
    "\n",
    "    s1 = res_to_series(\"S1\", r1, EPS_LIST)\n",
    "    s2 = res_to_series(\"S2\", r2, EPS_LIST)\n",
    "    s3 = res_to_series(\"S3\", r3, EPS_LIST)\n",
    "\n",
    "    df_all = pd.concat([s1, s2, s3], axis=1).T\n",
    "    df_mean = df_all.mean(axis=0)\n",
    "\n",
    "    print(f\"\\n==== Tabla de métricas por escenario {pretty_name} ====\")\n",
    "    print(df_all.round(6))\n",
    "    print(f\"\\n==== Medias (S1,S2,S3) {pretty_name} ====\")\n",
    "    print(df_mean.round(6))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3188f85-7c1b-484c-8f6e-7cc7e64bb571",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
