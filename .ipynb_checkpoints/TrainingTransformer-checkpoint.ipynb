{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b784a0ed-ac04-4d3a-af53-04c0fde70b99",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Descarga de índices bursátiles desde Yahoo Finances.\n",
    "# -- Sólo descomentar una vez, el resto de usos se pueden hacer desde el fichero descargado\n",
    "\"\"\"\n",
    "import yfinance as yf\n",
    "\n",
    "tickers = [\n",
    "    \"^GSPC\",\"^IXIC\",\"^DJI\",\"^RUT\",\n",
    "    \"^FTSE\",\"^GDAXI\",\"^FCHI\",\"^125904-USD-STRD\",\"^IBEX\",\n",
    "    \"^N225\",\"^HSI\",\"000001.SS\",\"^KS11\",\"^BSESN\",\n",
    "    \"^GSPTSE\",\"^BVSP\",\"^MXX\",\"^MERV\",\n",
    "    \"^AXJO\",\"^NZ50\",\n",
    "    \"ES=F\",\"NQ=F\",\"YM=F\",\"ZT=F\",\"^VIX\"\n",
    "]\n",
    "\n",
    "# Descarga de cierres diarios sin agrupar por ticker\n",
    "data = yf.download(\n",
    "    tickers,\n",
    "    start=\"2005-01-01\",\n",
    "    end=\"2025-01-02\"\n",
    ")\n",
    "\n",
    "cierres = data[\"Close\"]  # DataFrame con cada ticker como columna\n",
    "cierres.to_csv(\"./data/cierres_diarios_2005_2025.csv\")\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5cc34691-25a0-4a1f-9086-93ddab4ccdc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Uso de los datos descargados para entrenamiento\n",
    "# -- Asegurarse de enrutamiento y nombre de fichero correctos\n",
    "\n",
    "import pandas as pd\n",
    "df = pd.read_csv('./data/cierres_diarios_2005_2025n.csv', parse_dates=['Date'], index_col='Date')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4cd66cde-2833-4633-8201-741fd914285b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-18 12:51:27.498164: I metal_plugin/src/device/metal_device.cc:1154] Metal device set to: Apple M2\n",
      "2025-09-18 12:51:27.498194: I metal_plugin/src/device/metal_device.cc:296] systemMemory: 16.00 GB\n",
      "2025-09-18 12:51:27.498198: I metal_plugin/src/device/metal_device.cc:313] maxCacheSize: 5.33 GB\n",
      "2025-09-18 12:51:27.498214: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:305] Could not identify NUMA node of platform GPU ID 0, defaulting to 0. Your kernel may not have been built with NUMA support.\n",
      "2025-09-18 12:51:27.498224: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:271] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 0 MB memory) -> physical PluggableDevice (device: 0, name: METAL, pci bus id: <undefined>)\n"
     ]
    }
   ],
   "source": [
    "# Preparación de los datos\n",
    "\n",
    "# Eliminación de nulos\n",
    "df.ffill(inplace=True)\n",
    "df.bfill(inplace=True)\n",
    "\n",
    "# Split en crudo (¡antes de escalar!)\n",
    "n = len(df)\n",
    "train_raw = df.iloc[:int(n*0.7)]\n",
    "val_raw   = df.iloc[int(n*0.7):int(n*0.9)]\n",
    "test_raw  = df.iloc[int(n*0.9):]\n",
    "\n",
    "# Ajustar scaler SOLO con TRAIN\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from joblib import dump\n",
    "scaler = MinMaxScaler()\n",
    "scaler.fit(train_raw)  # <- fit solo con train\n",
    "\n",
    "# Transformar cada split con ese scaler\n",
    "import pandas as pd\n",
    "train_df = pd.DataFrame(scaler.transform(train_raw), index=train_raw.index, columns=train_raw.columns).astype(\"float32\")\n",
    "val_df   = pd.DataFrame(scaler.transform(val_raw),   index=val_raw.index,   columns=val_raw.columns).astype(\"float32\")\n",
    "test_df  = pd.DataFrame(scaler.transform(test_raw),  index=test_raw.index,  columns=test_raw.columns).astype(\"float32\")\n",
    "\n",
    "# Guardar scaler para escenarios S/M/L\n",
    "dump(scaler, \"scaler_modelos.joblib\")\n",
    "\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "def make_dataset(data, window_size, horizon, batch_size=32, shuffle=True):\n",
    "    ds = tf.keras.preprocessing.timeseries_dataset_from_array(\n",
    "        data=data.values,\n",
    "        targets=None,\n",
    "        sequence_length=window_size + horizon,\n",
    "        sequence_stride=1,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=shuffle\n",
    "    )\n",
    "    return ds.map(\n",
    "        lambda seq: (\n",
    "            tf.cast(seq[:, :window_size, :], tf.float32),\n",
    "            tf.cast(seq[:, window_size:, :], tf.float32)\n",
    "        )\n",
    "    )\n",
    "\n",
    "\n",
    "# Recreamos datasets con shuffle=False para val/test:\n",
    "train_dss = make_dataset(train_df, window_size=20,  horizon=1,  batch_size=32, shuffle=True)\n",
    "val_dss   = make_dataset(val_df,   window_size=20,  horizon=1,  batch_size=32, shuffle=False)\n",
    "test_dss  = make_dataset(test_df,  window_size=20,  horizon=1,  batch_size=32, shuffle=False)\n",
    "\n",
    "train_dsm = make_dataset(train_df, window_size=60,  horizon=5,  batch_size=32, shuffle=True)\n",
    "val_dsm   = make_dataset(val_df,   window_size=60,  horizon=5,  batch_size=32, shuffle=False)\n",
    "test_dsm  = make_dataset(test_df,  window_size=60,  horizon=5,  batch_size=32, shuffle=False)\n",
    "\n",
    "train_dsl = make_dataset(train_df, window_size=120, horizon=20, batch_size=32, shuffle=True)\n",
    "val_dsl   = make_dataset(val_df,   window_size=120, horizon=20, batch_size=32, shuffle=False)\n",
    "test_dsl  = make_dataset(test_df,  window_size=120, horizon=20, batch_size=32, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "087752d9-adb2-4895-8c39-acce55ec35e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-18 12:51:30.260079: W tensorflow/core/framework/local_rendezvous.cc:404] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Grid δ (calibrado con cuantiles |e| en VAL): [0.00853496789932251, 0.01, 0.017357412725687027, 0.02, 0.03049677610397339, 0.04116208553314208, 0.05, 0.1]\n",
      "\n",
      "=== Barrido Huber delta _S (Transformer) ===\n",
      "\n",
      "--- Entrenando δ=0.00853496789932251 ---\n",
      "Epoch 1/160\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-18 12:51:31.842861: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:117] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m114/114\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 81ms/step - log_cosh: 0.1747 - loss: 0.0042 - within_eps_0_005: 0.0076 - within_eps_0_01: 0.0144 - within_eps_0_02: 0.0283 - within_eps_0_05: 0.0695 - within_eps_0_1: 0.1379 - val_log_cosh: 0.0800 - val_loss: 0.0025 - val_within_eps_0_005: 0.0141 - val_within_eps_0_01: 0.0290 - val_within_eps_0_02: 0.0582 - val_within_eps_0_05: 0.1434 - val_within_eps_0_1: 0.2885\n",
      "Epoch 2/160\n",
      "\u001b[1m 75/114\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m2s\u001b[0m 70ms/step - log_cosh: 0.0988 - loss: 0.0031 - within_eps_0_005: 0.0087 - within_eps_0_01: 0.0175 - within_eps_0_02: 0.0339 - within_eps_0_05: 0.0869 - within_eps_0_1: 0.1729"
     ]
    }
   ],
   "source": [
    "# =========================================================\n",
    "# TRANSFORMER + Huber (sweep delta)\n",
    "# Selección de δ por métrica común: val_log_cosh\n",
    "# Grid de δ calibrado con cuantiles de |e| (baseline persistencia) en VAL\n",
    "# Guardado .keras (Flask-ready, compile=False)\n",
    "# =========================================================\n",
    "import os\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from typing import List, Tuple\n",
    "from tensorflow.keras import Model\n",
    "from tensorflow.keras.layers import (\n",
    "    Input, Dense, Dropout, LayerNormalization, Embedding,\n",
    "    MultiHeadAttention, Reshape\n",
    ")\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "\n",
    "# ---------- Scaler único (MinMax) ----------\n",
    "try:\n",
    "    scaler\n",
    "except NameError:\n",
    "    from joblib import load\n",
    "    scaler = load(\"scaler_modelos.joblib\")\n",
    "\n",
    "# ---------- Config ----------\n",
    "MODEL_DIR = \"./models_transformer_huber_sweep\"\n",
    "os.makedirs(MODEL_DIR, exist_ok=True)\n",
    "\n",
    "# EPS para métricas within (escala normalizada [0,1])\n",
    "EPS_LIST = [0.005, 0.01, 0.02, 0.05, 0.1]\n",
    "\n",
    "# ---------- Utils ----------\n",
    "def infer_shapes_from_dataset(ds: tf.data.Dataset) -> Tuple[int, int, int]:\n",
    "    for xb, yb in ds.take(1):\n",
    "        w = int(xb.shape[1]); f = int(xb.shape[2]); h = int(yb.shape[1])\n",
    "        return w, f, h\n",
    "    raise ValueError(\"Dataset vacío\")\n",
    "\n",
    "def _eps_tag(x: float) -> str:\n",
    "    s = f\"{x:.10g}\"\n",
    "    s = s.rstrip('0').rstrip('.') if '.' in s else s\n",
    "    return s.replace('.', '_')\n",
    "\n",
    "# ---------- Métricas ----------\n",
    "def log_cosh_metric(y_true, y_pred):\n",
    "    e = tf.cast(y_pred, tf.float32) - tf.cast(y_true, tf.float32)\n",
    "    ae = tf.abs(e)\n",
    "    return tf.reduce_mean(ae + tf.nn.softplus(-2.0 * ae) - tf.math.log(2.0))\n",
    "log_cosh_metric.__name__ = \"log_cosh\"\n",
    "\n",
    "def make_within_eps_vector_metric(eps_vec: np.ndarray, tag: str):\n",
    "    eps_tf = tf.constant(eps_vec.astype(np.float32), dtype=tf.float32)  # (F,)\n",
    "    def within_eps(y_true, y_pred):\n",
    "        diff = tf.abs(tf.cast(y_pred, tf.float32) - tf.cast(y_true, tf.float32))  # (B,H,F)\n",
    "        thr  = eps_tf[tf.newaxis, tf.newaxis, :]\n",
    "        hit  = tf.cast(diff <= thr, tf.float32)\n",
    "        return tf.reduce_mean(hit)\n",
    "    within_eps.__name__ = tag\n",
    "    return within_eps\n",
    "\n",
    "def build_within_metrics_minmax(scaler, eps_list: List[float], n_features: int):\n",
    "    if not hasattr(scaler, \"data_range_\"):\n",
    "        raise ValueError(\"Se esperaba MinMaxScaler con data_range_.\")\n",
    "    if len(scaler.data_range_) != n_features:\n",
    "        raise ValueError(\"scaler.data_range_ no coincide con n_features.\")\n",
    "    metrics = [log_cosh_metric]\n",
    "    for e in eps_list:\n",
    "        eps_vec = np.full((n_features,), float(e), dtype=np.float32)\n",
    "        metrics.append(make_within_eps_vector_metric(eps_vec, f\"within_eps_{_eps_tag(e)}\"))\n",
    "    return metrics\n",
    "\n",
    "def _get_metric_value_relaxed(res: dict, base_name: str):\n",
    "    \"\"\"\n",
    "    Devuelve res[base_name] si existe; si no, busca cualquier clave que empiece por base_name + '_'\n",
    "    (para cubrir sufijos que Keras añade como '_1', '_2', etc.). Si no hay match, devuelve NaN.\n",
    "    \"\"\"\n",
    "    if base_name in res and np.isfinite(res[base_name]):\n",
    "        return float(res[base_name])\n",
    "    # Busca con sufijo\n",
    "    for k, v in res.items():\n",
    "        if k.startswith(base_name + \"_\"):\n",
    "            try:\n",
    "                v = float(v)\n",
    "                if np.isfinite(v):\n",
    "                    return v\n",
    "            except Exception:\n",
    "                pass\n",
    "    return float(\"nan\")\n",
    "\n",
    "def compute_autc_from_results(res: dict, eps_list) -> float:\n",
    "    eps = np.array(sorted(eps_list), dtype=np.float64)\n",
    "    acc = []\n",
    "    missing = []\n",
    "    for e in eps:\n",
    "        base = f\"within_eps_{_eps_tag(e)}\"\n",
    "        val = _get_metric_value_relaxed(res, base)\n",
    "        acc.append(val)\n",
    "        if not np.isfinite(val):\n",
    "            missing.append(base)\n",
    "    acc = np.array(acc, dtype=np.float64)\n",
    "\n",
    "    # Debug útil si algo falla\n",
    "    if missing:\n",
    "        print(\"[AUTC] Aviso: no se encontraron métricas:\", missing)\n",
    "        print(\"[AUTC] Claves disponibles:\", sorted([k for k in res.keys() if \"within_eps_\" in k]))\n",
    "\n",
    "    mask = np.isfinite(acc)\n",
    "    if mask.sum() < 2:\n",
    "        return float(\"nan\")\n",
    "\n",
    "    return float(np.trapz(acc[mask], eps[mask]) / (eps[mask][-1] - eps[mask][0]))\n",
    "\n",
    "\n",
    "# ---------- Bloque Transformer ----------\n",
    "def transformer_encoder(x, d_model: int, num_heads: int, ff_dim: int, dropout: float):\n",
    "    attn_out = MultiHeadAttention(\n",
    "        num_heads=num_heads,\n",
    "        key_dim=d_model // num_heads,\n",
    "        dropout=dropout\n",
    "    )(x, x)\n",
    "    x = LayerNormalization(epsilon=1e-6)(x + Dropout(dropout)(attn_out))\n",
    "\n",
    "    ffn = Dense(ff_dim, activation='relu')(x)\n",
    "    ffn = Dropout(dropout)(ffn)\n",
    "    ffn = Dense(d_model)(ffn)\n",
    "    x = LayerNormalization(epsilon=1e-6)(x + Dropout(dropout)(ffn))\n",
    "    return x\n",
    "\n",
    "# ---------- Modelo Transformer ----------\n",
    "def build_transformer_point_model(window: int, n_features: int, horizon: int,\n",
    "                                  d_model: int = 128, num_heads: int = 4,\n",
    "                                  num_layers: int = 2, ff_dim: int = 256,\n",
    "                                  dropout: float = 0.1) -> Model:\n",
    "    inp = Input(shape=(window, n_features))\n",
    "    x = Dense(d_model)(inp)  # (B, W, d_model)\n",
    "\n",
    "    positions = tf.range(start=0, limit=window, delta=1)\n",
    "    pos_emb = Embedding(input_dim=window, output_dim=d_model)(positions)  # (W, d_model)\n",
    "    x = x + pos_emb  # broadcast sobre batch\n",
    "\n",
    "    for _ in range(num_layers):\n",
    "        x = transformer_encoder(x, d_model, num_heads, ff_dim, dropout)\n",
    "\n",
    "    x_last = x[:, -1, :]  # (B, d_model)\n",
    "\n",
    "    h = Dense(ff_dim, activation='relu')(x_last)\n",
    "    h = Dropout(dropout)(h)\n",
    "    h = Dense(horizon * n_features)(h)\n",
    "    out = Reshape((horizon, n_features))(h)\n",
    "    return Model(inp, out, name=f\"TRANSFORMER_POINT_H{horizon}_F{n_features}\")\n",
    "\n",
    "# ---------- Pérdida Huber ----------\n",
    "def make_huber_loss(delta: float):\n",
    "    base = tf.keras.losses.Huber(delta=float(delta))\n",
    "    def huber_loss(y_true, y_pred):\n",
    "        y_true = tf.cast(y_true, tf.float32); y_pred = tf.cast(y_pred, tf.float32)\n",
    "        return base(y_true, y_pred)\n",
    "    huber_loss.__name__ = f\"huber_delta_{_eps_tag(delta)}\"\n",
    "    return huber_loss\n",
    "\n",
    "# ---------- Calibración del grid de δ (cuantiles de |e| con baseline persistencia) ----------\n",
    "def estimate_error_quantiles_persistence(val_ds: tf.data.Dataset, max_batches: int = 256):\n",
    "    \"\"\"Cuantiles de |e| en VAL usando baseline de persistencia (y_hat = último paso repetido).\"\"\"\n",
    "    errs = []\n",
    "    taken = 0\n",
    "    for xb, yb in val_ds:\n",
    "        yhat = tf.repeat(xb[:, -1:, :], repeats=tf.shape(yb)[1], axis=1)  # (B,H,F)\n",
    "        e = tf.abs(tf.cast(yb, tf.float32) - tf.cast(yhat, tf.float32)).numpy().ravel()\n",
    "        errs.append(e); taken += 1\n",
    "        if taken >= max_batches:\n",
    "            break\n",
    "    if not errs:\n",
    "        return [0.01, 0.02, 0.05, 0.1]\n",
    "    e = np.concatenate(errs)\n",
    "    qs = np.quantile(e, [0.5, 0.75, 0.9, 0.95])  # p50, p75, p90, p95\n",
    "    return list(qs)\n",
    "\n",
    "def build_delta_grid(val_ds: tf.data.Dataset):\n",
    "    base = [0.01, 0.02, 0.05, 0.1]\n",
    "    qs = estimate_error_quantiles_persistence(val_ds, max_batches=256)\n",
    "    cand = sorted(set(base + qs))\n",
    "    cand = [float(np.clip(c, 1e-4, 0.5)) for c in cand]\n",
    "    cand = sorted(set(cand))\n",
    "    print(\"\\nGrid δ (calibrado con cuantiles |e| en VAL):\", cand)\n",
    "    return cand\n",
    "\n",
    "# ---------- Entrenamiento para un δ (callbacks en val_log_cosh) ----------\n",
    "def train_for_delta(train_ds: tf.data.Dataset,\n",
    "                    val_ds: tf.data.Dataset,\n",
    "                    delta: float,\n",
    "                    scenario_tag: str,\n",
    "                    scaler) -> tuple[Model, float, str]:\n",
    "    w, f, h = infer_shapes_from_dataset(train_ds)\n",
    "    model = build_transformer_point_model(w, f, h)\n",
    "    metrics = build_within_metrics_minmax(scaler, EPS_LIST, n_features=f)\n",
    "\n",
    "    model.compile(\n",
    "        optimizer=tf.keras.optimizers.Adam(learning_rate=3e-4, clipnorm=1.0),\n",
    "        loss=make_huber_loss(delta),\n",
    "        metrics=metrics\n",
    "    )\n",
    "\n",
    "    ckpt_path = os.path.join(\n",
    "        MODEL_DIR, f\"transformer_huber_w{w}_h{h}_delta{_eps_tag(delta)}{scenario_tag}.keras\"\n",
    "    )\n",
    "\n",
    "    callbacks = [\n",
    "        tf.keras.callbacks.TerminateOnNaN(),\n",
    "        EarlyStopping(monitor=\"val_log_cosh\", mode=\"min\", patience=15, restore_best_weights=True),\n",
    "        ModelCheckpoint(ckpt_path, monitor=\"val_log_cosh\", mode=\"min\", save_best_only=True)\n",
    "    ]\n",
    "\n",
    "    hist = model.fit(train_ds, validation_data=val_ds, epochs=160, callbacks=callbacks, verbose=1)\n",
    "    vlc = np.array(hist.history.get(\"val_log_cosh\", []), dtype=np.float32)\n",
    "    vlc = vlc[np.isfinite(vlc)]\n",
    "    best_val_logcosh = float(np.min(vlc)) if vlc.size > 0 else np.inf\n",
    "    return model, best_val_logcosh, ckpt_path\n",
    "\n",
    "# ---------- Barrido de deltas (selección por val_log_cosh) ----------\n",
    "def sweep_deltas(train_ds, val_ds, scenario_tag: str, scaler) -> tuple[Model, float, str]:\n",
    "    deltas = build_delta_grid(val_ds)\n",
    "    print(f\"\\n=== Barrido Huber delta {scenario_tag} (Transformer) ===\")\n",
    "    results = []  # (delta, best_val_logcosh, ckpt_path)\n",
    "\n",
    "    best_score = np.inf; best_ckpt=None; best_delta=None; best_model=None\n",
    "    for d in deltas:\n",
    "        print(f\"\\n--- Entrenando δ={d} ---\")\n",
    "        model, val_logcosh, ckpt_path = train_for_delta(train_ds, val_ds, d, scenario_tag, scaler)\n",
    "        print(f\"  -> val_log_cosh(min) δ={d}: {val_logcosh:.12f}\")\n",
    "        results.append((d, val_logcosh, ckpt_path))\n",
    "        if val_logcosh < best_score:\n",
    "            best_score, best_ckpt, best_delta, best_model = val_logcosh, ckpt_path, d, model\n",
    "\n",
    "    if best_ckpt:\n",
    "        best_model = tf.keras.models.load_model(best_ckpt, compile=False)\n",
    "\n",
    "    results_sorted = sorted(results, key=lambda x: x[1])\n",
    "    print(\"\\nTabla val_log_cosh(min) por δ (orden asc):\")\n",
    "    for d, v, _ in results_sorted:\n",
    "        print(f\"  δ={d:>5}: {v:.12f}\")\n",
    "    print(f\"\\n>>> Mejor δ por val_log_cosh: {best_delta} (val_log_cosh={best_score:.12f})\")\n",
    "    return best_model, best_delta, best_ckpt\n",
    "\n",
    "# ---------- Evaluación en TEST ----------\n",
    "def evaluate_on_test(model: tf.keras.Model, ds: tf.data.Dataset, best_delta: float, scaler):\n",
    "    if (model is None) or (best_delta is None) or (not np.isfinite(best_delta)):\n",
    "        print(\"  [AVISO] No se pudo entrenar un modelo válido.\")\n",
    "        return\n",
    "    _, f, _ = infer_shapes_from_dataset(ds)\n",
    "    metrics = build_within_metrics_minmax(scaler, EPS_LIST, n_features=f)\n",
    "    model.compile(optimizer=\"adam\", loss=make_huber_loss(best_delta), metrics=metrics)\n",
    "\n",
    "    res = model.evaluate(ds, return_dict=True, verbose=0)\n",
    "    print(\"  loss (Huber):              {:.6f}\".format(res.get(\"loss\", float(\"nan\"))))\n",
    "    print(\"  log_cosh:                  {:.6f}\".format(res.get(\"log_cosh\", float(\"nan\"))))\n",
    "    for e in EPS_LIST:\n",
    "        key = f\"within_eps_{_eps_tag(e)}\"\n",
    "        print(f\"  {key:26s}: {res.get(key, float('nan')):.6f}\")\n",
    "    autc = compute_autc_from_results(res, EPS_LIST)\n",
    "    eps_min, eps_max = float(min(EPS_LIST)), float(max(EPS_LIST))\n",
    "    print(f\"  AUTC[{eps_min:.3f}–{eps_max:.3f}]:         {autc:.6f}\")\n",
    "\n",
    "# ================== EJECUCIÓN: TRES ESCENARIOS ==================\n",
    "# Usa tus datasets (sin shuffle en val/test):\n",
    "#   train_dss/val_dss/test_dss, train_dsm/val_dsm/test_dsm, train_dsl/val_dsl/test_dsl\n",
    "\n",
    "# Escenario S (20→1)\n",
    "model_s, delta_s, path_s = sweep_deltas(train_dss, val_dss, scenario_tag=\"_S\", scaler=scaler)\n",
    "print(\"\\nResultados TEST - Escenario S (Transformer)\")\n",
    "evaluate_on_test(model_s, test_dss, best_delta=delta_s, scaler=scaler)\n",
    "\n",
    "# Escenario M (60→5)\n",
    "model_m, delta_m, path_m = sweep_deltas(train_dsm, val_dsm, scenario_tag=\"_M\", scaler=scaler)\n",
    "print(\"\\nResultados TEST - Escenario M (Transformer)\")\n",
    "evaluate_on_test(model_m, test_dsm, best_delta=delta_m, scaler=scaler)\n",
    "\n",
    "# Escenario L (120→20)\n",
    "model_l, delta_l, path_l = sweep_deltas(train_dsl, val_dsl, scenario_tag=\"_L\", scaler=scaler)\n",
    "print(\"\\nResultados TEST - Escenario L (Transformer)\")\n",
    "evaluate_on_test(model_l, test_dsl, best_delta=delta_l, scaler=scaler)\n",
    "\n",
    "print(\"\\n=== Mejor δ por escenario (Transformer, métrica común: val_log_cosh) ===\")\n",
    "print(f\"  S: {delta_s}  -> {path_s}\")\n",
    "print(f\"  M: {delta_m}  -> {path_m}\")\n",
    "print(f\"  L: {delta_l}  -> {path_l}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d2e4eef-889c-4f81-b927-8f1dfe137bb1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
